#!/usr/bin/env python3
"""Train ReLU CNN on ImageNet using ModelTrainer"""

import torch
from datasets import load_from_disk
from transformers import TrainingArguments

# Import from shared_utils package
from shared_utils import ModelTrainer

from prelu_cnn import CNN, CNNTrainer

def main():
    """Train ReLU CNN on ImageNet."""
    
    print("ğŸš€ Training ReLU CNN on ImageNet")
    print("=" * 50)
    
    # Check CUDA availability
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    if device.type == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Load preprocessed ImageNet dataset from disk
    print("ğŸ”„ Loading preprocessed datasets from disk...")
    
    # Load training dataset
    dataset_path = "./processed_datasets/imagenet_processor"
    dataset = load_from_disk(dataset_path)
    
    train_dataset = dataset["train"]
    eval_dataset = dataset["validation"]
    
    print(f"âœ… Loaded preprocessed datasets from disk")
    print(f"âœ… Training samples: {len(train_dataset):,}")
    print(f"âœ… Validation samples: {len(eval_dataset):,}")
    
    use_prelu = False
    
    # Create ReLU CNN model
    print(f"\nğŸ—ï¸ Creating ReLU CNN model ({1000} classes)...")
    model = CNN(
        use_prelu=use_prelu,
        use_builtin_conv=True,  # Use fast PyTorch convolutions
        num_classes=1000
    )
    
    # Move model to GPU
    model = model.to(device)
    print(f"âœ… Model moved to device: {device}")
    
    # Verify model is on GPU
    if next(model.parameters()).device != device:
        print(f"âŒ Warning: Model parameters are on {next(model.parameters()).device}, expected {device}")
    else:
        print(f"âœ… Model parameters confirmed on {device}")
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š Model Statistics:")
    print(f"  Total parameters: {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")
    
    # Create training arguments
    training_args = TrainingArguments(
        output_dir=f"./results/cnn_results_{'prelu' if use_prelu else 'relu'}",
        num_train_epochs=90,  # More epochs for better convergence
        per_device_train_batch_size=64,  # Reduced for stability
        per_device_eval_batch_size=64,
        learning_rate=1e-3,
        weight_decay=1e-4,
        warmup_steps=1000,  # Warmup for better training stability
        gradient_accumulation_steps=4,  # Reduced for more frequent updates
        eval_steps=25,
        logging_steps=25,
        save_steps=500,
        seed=42,
        logging_dir="./logs/logs",
        # Fix for custom dataset format
        remove_unused_columns=False,
        # Parallel data loading
        dataloader_num_workers=4,
        # Optimizer and scheduler settings
        optim="adamw_torch",  # Explicit optimizer
        lr_scheduler_type="cosine",  # Cosine annealing scheduler
        max_grad_norm=1.0,  # Gradient clipping
        eval_strategy="steps",
        save_strategy="steps",
        logging_strategy="steps",
        # Model saving
        save_total_limit=3,  # Keep only 3 best checkpoints
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        prediction_loss_only=False,
        label_names=["labels"], # need this to get eval_loss
    )
    
    print(f"\nâš™ï¸ Training Configuration:")
    print(f"  Epochs: {training_args.num_train_epochs}")
    print(f"  Batch size: {training_args.per_device_train_batch_size}")
    print(f"  Learning rate: {training_args.learning_rate}")
    print(f"  Weight decay: {training_args.weight_decay}")
    print(f"  Warmup steps: {training_args.warmup_steps}")
    print(f"  LR scheduler: {training_args.lr_scheduler_type}")
    print(f"  Optimizer: {training_args.optim}")
    print(f"  Gradient clipping: {training_args.max_grad_norm}")
    print(f"  Evaluation steps: {training_args.eval_steps}")
    print(f"  Output directory: {training_args.output_dir}")
    print(f"  Remove unused columns: {training_args.remove_unused_columns}")
    print(f"  Dataloader workers: {training_args.dataloader_num_workers}")
    
    # Create trainer using ModelTrainer
    print(f"\nğŸ‹ï¸ Setting up trainer...")
    trainer = ModelTrainer(
        model=model,
        training_args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        trainer_class=CNNTrainer,
    )
    
    # Run training
    trainer.run()
    
    print(f"ğŸ’¾ Model saved to: {training_args.output_dir}")
    print(f"\nâœ… Training completed successfully!")

if __name__ == "__main__":
    main() 