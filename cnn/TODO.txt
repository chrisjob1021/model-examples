
  Training Recommendations:

  1. Learning Rate Schedule

  - Keep cosine but use full cycle (num_cycles: 1.0 instead of 0.25)
  - Consider cosine restarts for better exploration
  - Set min_lr_rate: 0.001 (0.1% of initial LR, not 10%)

  2. Optimizer Upgrades

  - AdamW often converges faster: optim="adamw", learning_rate=0.001
  - Or LAMB for large batch training
  - If staying with SGD, add Nesterov momentum: optim_args="momentum=0.9,nesterov=True"

  3. Gradient Clipping

  - Add mild clipping: max_grad_norm=1.0 (helps stability)

  4. Warmup

  - Reduce to 3-5% of total steps (not 10%)

  5. Label Smoothing

  - Add label_smoothing_factor=0.1 in TrainingArguments (currently hardcoded)

  6. Mixed Precision

  - Add fp16=True or bf16=True for faster training

  7. Data Augmentation

  - Reduce RandomErasing to p=0.1 (25% is aggressive)
  - Add MixUp or CutMix for better regularization

  8. Learning Rate Scaling

  - With larger batch size, scale LR: lr = 0.01 * (batch_size / 256)

  If you run strong aug (RandAugment, Mixup α≈0.2–0.4, CutMix β≈1.0), long cosine (hundreds of epochs), momentum 0.9–0.95, WD 1e-4–5e-4, and EMA (0.999–0.9999), you’ve effectively smoothed and preconditioned the landscape. SGD’s implicit bias toward flat, large-margin solutions dominates, so it often beats Adam/AdamW on final accuracy—even if Adam is faster early on.
