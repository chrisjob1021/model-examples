
  Modern Recommendations:

  1. Learning Rate Schedule

  - Keep cosine but use full cycle (num_cycles: 1.0 instead of 0.25)
  - Consider cosine restarts for better exploration
  - Set min_lr_rate: 0.001 (0.1% of initial LR, not 10%)

  2. Optimizer Upgrades

  - AdamW often converges faster: optim="adamw", learning_rate=0.001
  - Or LAMB for large batch training
  - If staying with SGD, add Nesterov momentum: optim_args="momentum=0.9,nesterov=True"

  3. Gradient Clipping

  - Add mild clipping: max_grad_norm=1.0 (helps stability)

  4. Warmup

  - Reduce to 3-5% of total steps (not 10%)
  - Linear warmup is fine

  5. Label Smoothing

  - Add label_smoothing_factor=0.1 in TrainingArguments
  - Helps generalization significantly

  6. Mixed Precision

  - Add fp16=True or bf16=True for faster training
  - Allows larger batch sizes

  7. Data Augmentation

  - Your RandAugment is good but consider:
    - Reduce RandomErasing to p=0.1 (25% is aggressive)
    - Add MixUp or CutMix for better regularization

  8. Learning Rate Scaling

  - With larger batch size, scale LR: lr = 0.01 * (batch_size / 256)
