{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfe7e04eb1e4193b6ec833bd8aa83f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "train_dataset = load_dataset(\"imagenet-1k\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"imagenet-1k\", split=\"validation\")\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the data augmentation and preprocessing pipeline for training images\n",
    "train_transform = T.Compose([\n",
    "    T.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),  # Ensure 3 channels (convert grayscale to RGB)\n",
    "    T.RandomResizedCrop(224, scale=(0.08, 1.0)),      # Randomly crop and resize to 224x224 (simulates zoom/scale)\n",
    "    T.RandomHorizontalFlip(),                         # Randomly flip images horizontally (augmentation)\n",
    "    T.RandAugment(num_ops=2, magnitude=9),            # Apply 2 random augmentations with magnitude 9 (extra augmentation)\n",
    "    T.ToTensor(),                                     # Convert PIL Image or numpy.ndarray to tensor and scale to [0, 1]\n",
    "    T.Normalize(mean, std),                           # Normalize using ImageNet mean and std\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1)),       # Randomly erase a rectangle region (extra augmentation, 25% chance)\n",
    "])\n",
    "\n",
    "# Define the preprocessing pipeline for evaluation images (no heavy augmentation)\n",
    "eval_transform = T.Compose([\n",
    "    T.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),  # Ensure 3 channels (convert grayscale to RGB)\n",
    "    T.Resize(256),                                    # Resize shorter side to 256 pixels\n",
    "    T.CenterCrop(224),                                # Crop the center 224x224 region\n",
    "    T.ToTensor(),                                     # Convert to tensor and scale to [0, 1]\n",
    "    T.Normalize(mean, std),                           # Normalize using ImageNet mean and std\n",
    "])\n",
    "\n",
    "def train_transform_fn(examples):\n",
    "    # Handle both single examples and batches\n",
    "    if isinstance(examples['image'], list):\n",
    "        # Batch processing\n",
    "        examples[\"pixel_values\"] = [train_transform(image) for image in examples[\"image\"]]\n",
    "    else:\n",
    "        # Single example processing  \n",
    "        examples[\"pixel_values\"] = train_transform(examples[\"image\"])\n",
    "    \n",
    "    # Remove the original image to avoid DataLoader issues\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "def eval_transform_fn(examples):\n",
    "    # Handle both single examples and batches\n",
    "    if isinstance(examples['image'], list):\n",
    "        # Batch processing\n",
    "        examples[\"pixel_values\"] = [eval_transform(image) for image in examples[\"image\"]]\n",
    "    else:\n",
    "        # Single example processing\n",
    "        examples[\"pixel_values\"] = eval_transform(examples[\"image\"])\n",
    "    \n",
    "    # Remove the original image to avoid DataLoader issues\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# Now you can use these with with_transform()\n",
    "train_dataset = train_dataset.with_transform(train_transform_fn)\n",
    "eval_dataset = eval_dataset.with_transform(eval_transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=817x363>,\n",
       "  'label': 726,\n",
       "  'pixel_values': tensor([[[-0.1828, -0.1657, -0.1657,  ...,  0.1254,  0.1083,  0.1254],\n",
       "           [-0.1828, -0.1828, -0.1657,  ...,  0.1254,  0.1083,  0.1083],\n",
       "           [-0.1657, -0.1657, -0.1657,  ...,  0.1083,  0.1083,  0.1083],\n",
       "           ...,\n",
       "           [ 0.2111,  0.1939,  0.1939,  ...,  0.2796,  0.2453,  0.2624],\n",
       "           [ 0.1939,  0.1939,  0.1939,  ...,  0.2624,  0.2624,  0.2624],\n",
       "           [ 0.1939,  0.1939,  0.2111,  ...,  0.2796,  0.2796,  0.2796]],\n",
       "  \n",
       "          [[ 0.0126,  0.0301,  0.0301,  ...,  0.3277,  0.3102,  0.3277],\n",
       "           [ 0.0301,  0.0301,  0.0301,  ...,  0.3102,  0.2927,  0.2927],\n",
       "           [ 0.0126,  0.0126,  0.0126,  ...,  0.2927,  0.2927,  0.2927],\n",
       "           ...,\n",
       "           [ 0.3452,  0.3277,  0.3277,  ...,  0.4153,  0.3978,  0.4153],\n",
       "           [ 0.3277,  0.3277,  0.3277,  ...,  0.4153,  0.4153,  0.4153],\n",
       "           [ 0.3277,  0.3452,  0.3627,  ...,  0.4328,  0.4328,  0.4153]],\n",
       "  \n",
       "          [[ 0.1651,  0.1999,  0.1999,  ...,  0.4962,  0.4788,  0.4962],\n",
       "           [ 0.1999,  0.1999,  0.2173,  ...,  0.5136,  0.4962,  0.4962],\n",
       "           [ 0.1825,  0.1999,  0.2173,  ...,  0.4962,  0.4962,  0.4962],\n",
       "           ...,\n",
       "           [ 0.4962,  0.4788,  0.4788,  ...,  0.5659,  0.5485,  0.5659],\n",
       "           [ 0.4962,  0.4788,  0.4962,  ...,  0.5659,  0.5659,  0.5659],\n",
       "           [ 0.4962,  0.4962,  0.5136,  ...,  0.5834,  0.5834,  0.5834]]])},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=408x500>,\n",
       "  'label': 91,\n",
       "  'pixel_values': tensor([[[-2.0152, -1.7583, -1.9638,  ..., -1.8610, -1.7069, -2.0152],\n",
       "           [-2.0152, -1.7583, -1.9638,  ..., -1.8439, -1.6898, -2.0152],\n",
       "           [-2.0152, -1.7754, -1.9809,  ..., -1.8268, -1.6727, -2.0152],\n",
       "           ...,\n",
       "           [-1.9638, -0.7993, -0.3541,  ..., -0.0972, -0.5938, -1.9295],\n",
       "           [-1.9638, -0.7822, -0.3369,  ..., -0.4397, -0.7137, -1.9295],\n",
       "           [-1.9638, -0.7650, -0.2856,  ..., -0.3369, -0.6794, -1.9467]],\n",
       "  \n",
       "          [[-1.9307, -1.5805, -1.7031,  ..., -1.5280, -1.4755, -1.9307],\n",
       "           [-1.9132, -1.5980, -1.7381,  ..., -1.5280, -1.4755, -1.9132],\n",
       "           [-1.9132, -1.6155, -1.7556,  ..., -1.5105, -1.4930, -1.9307],\n",
       "           ...,\n",
       "           [-1.8606, -0.5476,  0.0126,  ..., -0.4951, -0.8277, -1.8957],\n",
       "           [-1.8782, -0.5126,  0.0651,  ..., -0.8277, -0.9153, -1.8606],\n",
       "           [-1.8782, -0.4951,  0.1176,  ..., -0.7577, -0.8803, -1.8782]],\n",
       "  \n",
       "          [[-1.6824, -1.5604, -1.7696,  ..., -1.7870, -1.5256, -1.6650],\n",
       "           [-1.7173, -1.5430, -1.7870,  ..., -1.7870, -1.4907, -1.6824],\n",
       "           [-1.7173, -1.5256, -1.7870,  ..., -1.7870, -1.4907, -1.6999],\n",
       "           ...,\n",
       "           [-1.6999, -1.5081, -1.7870,  ..., -0.8458, -0.9678, -1.6824],\n",
       "           [-1.6999, -1.4907, -1.8044,  ..., -1.1421, -1.0550, -1.6650],\n",
       "           [-1.6999, -1.4907, -1.7870,  ..., -1.0724, -1.0201, -1.6824]]])})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0], eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([3, 224, 224]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['pixel_values'].shape, eval_dataset[0]['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RGB conversion with actual grayscale image...\n",
      "Original grayscale image mode: L\n",
      "Original grayscale image size: (400, 300)\n",
      "\n",
      "Testing train transform on grayscale image:\n",
      "‚úÖ Train transform result shape: torch.Size([3, 224, 224])\n",
      "‚úÖ Successfully converted to 3 channels\n",
      "‚úÖ All 3 channels identical (as expected): False\n",
      "\n",
      "Testing eval transform on grayscale image:\n",
      "‚úÖ Eval transform result shape: torch.Size([3, 224, 224])\n",
      "‚úÖ Successfully converted to 3 channels\n",
      "‚úÖ All 3 channels identical (as expected): False\n",
      "\n",
      "üéØ Grayscale to RGB conversion test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test with actual grayscale image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Testing RGB conversion with actual grayscale image...\")\n",
    "\n",
    "# Create a test grayscale image\n",
    "gray_array = np.random.randint(0, 256, (300, 400), dtype=np.uint8)\n",
    "grayscale_image = Image.fromarray(gray_array, mode='L')  # 'L' mode = grayscale\n",
    "\n",
    "print(f\"Original grayscale image mode: {grayscale_image.mode}\")\n",
    "print(f\"Original grayscale image size: {grayscale_image.size}\")\n",
    "\n",
    "# Test train transform on grayscale image\n",
    "print(\"\\nTesting train transform on grayscale image:\")\n",
    "try:\n",
    "    train_result = train_transform(grayscale_image)\n",
    "    print(f\"‚úÖ Train transform result shape: {train_result.shape}\")\n",
    "    print(f\"‚úÖ Successfully converted to {train_result.shape[0]} channels\")\n",
    "    \n",
    "    # Verify all 3 channels have the same values (since it was grayscale)\n",
    "    channel_equality = torch.allclose(train_result[0], train_result[1]) and torch.allclose(train_result[1], train_result[2])\n",
    "    print(f\"‚úÖ All 3 channels identical (as expected): {channel_equality}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in train transform: {e}\")\n",
    "\n",
    "# Test eval transform on grayscale image  \n",
    "print(\"\\nTesting eval transform on grayscale image:\")\n",
    "try:\n",
    "    eval_result = eval_transform(grayscale_image)\n",
    "    print(f\"‚úÖ Eval transform result shape: {eval_result.shape}\")\n",
    "    print(f\"‚úÖ Successfully converted to {eval_result.shape[0]} channels\")\n",
    "    \n",
    "    # Verify all 3 channels have the same values (since it was grayscale)\n",
    "    channel_equality = torch.allclose(eval_result[0], eval_result[1]) and torch.allclose(eval_result[1], eval_result[2])\n",
    "    print(f\"‚úÖ All 3 channels identical (as expected): {channel_equality}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in eval transform: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Grayscale to RGB conversion test completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
