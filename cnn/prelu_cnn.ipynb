{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed674018e354099b5dad5ca7159715c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision.transforms as T\n",
    "from train_cnn_imagenet import SafeImageNetDataset\n",
    "\n",
    "train_dataset = load_dataset(\"imagenet-1k\", split=\"train\").shuffle(seed=42).select(range(100))\n",
    "eval_dataset = load_dataset(\"imagenet-1k\", split=\"validation\").shuffle(seed=42).select(range(100))\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the data augmentation and preprocessing pipeline for training images\n",
    "train_transform = T.Compose([\n",
    "    T.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),  # Ensure 3 channels (convert grayscale to RGB)\n",
    "    T.RandomResizedCrop(224, scale=(0.08, 1.0)),      # Randomly crop and resize to 224x224 (simulates zoom/scale)\n",
    "    T.RandomHorizontalFlip(),                         # Randomly flip images horizontally (augmentation)\n",
    "    T.RandAugment(num_ops=2, magnitude=9),            # Apply 2 random augmentations with magnitude 9 (extra augmentation)\n",
    "    T.ToTensor(),                                     # Convert PIL Image or numpy.ndarray to tensor and scale to [0, 1]\n",
    "    T.Normalize(mean, std),                           # Normalize using ImageNet mean and std\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1)),       # Randomly erase a rectangle region (extra augmentation, 25% chance)\n",
    "])\n",
    "\n",
    "# Define the preprocessing pipeline for evaluation images (no heavy augmentation)\n",
    "eval_transform = T.Compose([\n",
    "    T.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),  # Ensure 3 channels (convert grayscale to RGB)\n",
    "    T.Resize(256),                                    # Resize shorter side to 256 pixels\n",
    "    T.CenterCrop(224),                                # Crop the center 224x224 region\n",
    "    T.ToTensor(),                                     # Convert to tensor and scale to [0, 1]\n",
    "    T.Normalize(mean, std),                           # Normalize using ImageNet mean and std\n",
    "])\n",
    "\n",
    "def train_transform_fn(examples):\n",
    "    # Handle both single examples and batches\n",
    "    if isinstance(examples['image'], list):\n",
    "        # Batch processing\n",
    "        examples[\"pixel_values\"] = [train_transform(image) for image in examples[\"image\"]]\n",
    "    else:\n",
    "        # Single example processing  \n",
    "        examples[\"pixel_values\"] = train_transform(examples[\"image\"])\n",
    "\n",
    "    # Remove the original image to avoid DataLoader issues\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "def eval_transform_fn(examples):\n",
    "    # Handle both single examples and batches\n",
    "    if isinstance(examples['image'], list):\n",
    "        # Batch processing\n",
    "        examples[\"pixel_values\"] = [eval_transform(image) for image in examples[\"image\"]]\n",
    "    else:\n",
    "        # Single example processing\n",
    "        examples[\"pixel_values\"] = eval_transform(examples[\"image\"])\n",
    "\n",
    "    # Remove the original image to avoid DataLoader issues\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# Wrap datasets with safe wrapper to handle EXIF errors on-demand\n",
    "train_dataset = SafeImageNetDataset(train_dataset, train_transform_fn)\n",
    "eval_dataset = SafeImageNetDataset(eval_dataset, eval_transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'label': 126,\n",
       "  'pixel_values': tensor([[[-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.1290],\n",
       "           [-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.2489],\n",
       "           [-2.1179, -2.1179, -2.1179,  ...,  2.2489,  2.2489,  2.2489],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "          [[-2.0357, -2.0357, -2.0357,  ...,  1.8859,  1.7283,  1.5007],\n",
       "           [-2.0357, -2.0357, -2.0357,  ...,  1.9209,  1.8158,  1.6583],\n",
       "           [-2.0357, -2.0357, -2.0357,  ...,  2.0259,  1.9734,  1.9034],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "          [[-1.8044, -1.8044, -1.8044,  ...,  1.6117,  1.3851,  1.1411],\n",
       "           [-1.8044, -1.8044, -1.8044,  ...,  1.6291,  1.4722,  1.2805],\n",
       "           [-1.8044, -1.8044, -1.8044,  ...,  1.6988,  1.6117,  1.5245],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])},\n",
       " {'label': 348,\n",
       "  'pixel_values': tensor([[[ 1.6838,  1.5639,  1.5639,  ...,  0.1254,  0.1254,  0.0912],\n",
       "           [ 1.2728,  1.1700,  1.2043,  ...,  0.0056, -0.0458, -0.0458],\n",
       "           [ 0.8447,  0.7248,  0.6221,  ..., -0.0801, -0.1999, -0.2513],\n",
       "           ...,\n",
       "           [-0.1486, -0.2171, -0.1999,  ...,  1.4440,  1.4098,  0.6221],\n",
       "           [-0.1486, -0.1657, -0.1657,  ...,  1.1872,  1.2728,  0.4508],\n",
       "           [-0.0801, -0.0972, -0.2171,  ...,  0.8447,  0.9474,  0.4851]],\n",
       "  \n",
       "          [[ 1.7983,  1.6758,  1.6758,  ...,  0.1702,  0.1527,  0.1352],\n",
       "           [ 1.3957,  1.2731,  1.2906,  ...,  0.0476,  0.0126,  0.0126],\n",
       "           [ 0.9055,  0.8004,  0.6779,  ..., -0.0749, -0.1625, -0.1975],\n",
       "           ...,\n",
       "           [-0.2850, -0.3200, -0.3200,  ...,  1.0455,  1.0280,  0.1877],\n",
       "           [-0.3025, -0.3200, -0.2850,  ...,  0.7829,  0.9055,  0.0651],\n",
       "           [-0.2675, -0.2850, -0.3725,  ...,  0.4328,  0.5903,  0.1176]],\n",
       "  \n",
       "          [[ 1.4374,  1.2805,  1.2805,  ..., -0.2707, -0.2707, -0.2881],\n",
       "           [ 0.9668,  0.8622,  0.8971,  ..., -0.3927, -0.4275, -0.4275],\n",
       "           [ 0.4614,  0.3568,  0.2696,  ..., -0.5495, -0.6193, -0.6715],\n",
       "           ...,\n",
       "           [-0.8458, -0.8633, -0.8284,  ...,  0.1476,  0.0605, -0.8981],\n",
       "           [-0.8807, -0.8284, -0.7936,  ..., -0.0441, -0.0267, -1.0201],\n",
       "           [-0.7761, -0.7064, -0.8110,  ..., -0.3230, -0.3055, -0.8807]]])})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0], eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([3, 224, 224]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['pixel_values'].shape, eval_dataset[0]['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RGB conversion with actual grayscale image...\n",
      "Original grayscale image mode: L\n",
      "Original grayscale image size: (400, 300)\n",
      "\n",
      "Testing train transform on grayscale image:\n",
      "‚úÖ Train transform result shape: torch.Size([3, 224, 224])\n",
      "‚úÖ Successfully converted to 3 channels\n",
      "‚úÖ All 3 channels identical (as expected): False\n",
      "\n",
      "Testing eval transform on grayscale image:\n",
      "‚úÖ Eval transform result shape: torch.Size([3, 224, 224])\n",
      "‚úÖ Successfully converted to 3 channels\n",
      "‚úÖ All 3 channels identical (as expected): False\n",
      "\n",
      "üéØ Grayscale to RGB conversion test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test with actual grayscale image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Testing RGB conversion with actual grayscale image...\")\n",
    "\n",
    "# Create a test grayscale image\n",
    "gray_array = np.random.randint(0, 256, (300, 400), dtype=np.uint8)\n",
    "grayscale_image = Image.fromarray(gray_array, mode='L')  # 'L' mode = grayscale\n",
    "\n",
    "print(f\"Original grayscale image mode: {grayscale_image.mode}\")\n",
    "print(f\"Original grayscale image size: {grayscale_image.size}\")\n",
    "\n",
    "# Test train transform on grayscale image\n",
    "print(\"\\nTesting train transform on grayscale image:\")\n",
    "try:\n",
    "    train_result = train_transform(grayscale_image)\n",
    "    print(f\"‚úÖ Train transform result shape: {train_result.shape}\")\n",
    "    print(f\"‚úÖ Successfully converted to {train_result.shape[0]} channels\")\n",
    "    \n",
    "    # Verify all 3 channels have the same values (since it was grayscale)\n",
    "    channel_equality = torch.allclose(train_result[0], train_result[1]) and torch.allclose(train_result[1], train_result[2])\n",
    "    print(f\"‚úÖ All 3 channels identical (as expected): {channel_equality}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in train transform: {e}\")\n",
    "\n",
    "# Test eval transform on grayscale image  \n",
    "print(\"\\nTesting eval transform on grayscale image:\")\n",
    "try:\n",
    "    eval_result = eval_transform(grayscale_image)\n",
    "    print(f\"‚úÖ Eval transform result shape: {eval_result.shape}\")\n",
    "    print(f\"‚úÖ Successfully converted to {eval_result.shape[0]} channels\")\n",
    "    \n",
    "    # Verify all 3 channels have the same values (since it was grayscale)\n",
    "    channel_equality = torch.allclose(eval_result[0], eval_result[1]) and torch.allclose(eval_result[1], eval_result[2])\n",
    "    print(f\"‚úÖ All 3 channels identical (as expected): {channel_equality}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in eval transform: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Grayscale to RGB conversion test completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100 items from train_dataset...\n",
      "  Processed 20/100 samples...\n",
      "  Processed 40/100 samples...\n",
      "  Processed 60/100 samples...\n",
      "  Processed 80/100 samples...\n",
      "  Processed 100/100 samples...\n",
      "\n",
      "üìä train_dataset pixel value statistics:\n",
      "  Shape: (100, 3, 224, 224)\n",
      "  Min: -2.1179\n",
      "  Max: 2.6400\n",
      "  Mean: -0.1905\n",
      "  Std: 1.2783\n",
      "  ‚úÖ Pixel values in expected normalized range\n",
      "Sampling 100 items from eval_dataset...\n",
      "  Processed 20/100 samples...\n",
      "  Processed 40/100 samples...\n",
      "  Processed 60/100 samples...\n",
      "  Processed 80/100 samples...\n",
      "  Processed 100/100 samples...\n",
      "\n",
      "üìä eval_dataset pixel value statistics:\n",
      "  Shape: (100, 3, 224, 224)\n",
      "  Min: -2.1179\n",
      "  Max: 2.6400\n",
      "  Mean: -0.0460\n",
      "  Std: 1.1827\n",
      "  ‚úÖ Pixel values in expected normalized range\n"
     ]
    }
   ],
   "source": [
    "# Check pixel value range after normalization for both train and eval sets\n",
    "\n",
    "def check_pixel_range(dataset, name=\"dataset\"):\n",
    "    \"\"\"Check pixel value range for any dataset type.\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        # Sample up to 100 items from the dataset\n",
    "        sample_size = min(100, len(dataset))\n",
    "        arr = []\n",
    "        \n",
    "        print(f\"Sampling {sample_size} items from {name}...\")\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            try:\n",
    "                sample = dataset[i]\n",
    "                \n",
    "                # Handle different sample formats\n",
    "                if isinstance(sample, dict):\n",
    "                    if 'pixel_values' in sample:\n",
    "                        pixel_values = sample['pixel_values']\n",
    "                    elif 'image' in sample:\n",
    "                        # Apply transform if available\n",
    "                        if hasattr(dataset, 'transform_fn') and dataset.transform_fn:\n",
    "                            sample = dataset.transform_fn(sample)\n",
    "                            pixel_values = sample['pixel_values']\n",
    "                        else:\n",
    "                            continue  # Skip if no transform available\n",
    "                    else:\n",
    "                        continue  # Skip if no pixel data found\n",
    "                elif isinstance(sample, tuple) and len(sample) >= 1:\n",
    "                    pixel_values = sample[0]  # Assume first element is image\n",
    "                else:\n",
    "                    continue  # Skip unknown format\n",
    "                \n",
    "                # Convert to numpy if it's a tensor\n",
    "                if torch.is_tensor(pixel_values):\n",
    "                    pixel_values = pixel_values.cpu().numpy()\n",
    "                \n",
    "                arr.append(pixel_values)\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 20 == 0:\n",
    "                    print(f\"  Processed {i + 1}/{sample_size} samples...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Error processing sample {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not arr:\n",
    "            print(f\"‚ùå No valid pixel data found in {name}\")\n",
    "            return\n",
    "            \n",
    "        # Stack all pixel values\n",
    "        arr = np.stack(arr)\n",
    "        \n",
    "        print(f\"\\nüìä {name} pixel value statistics:\")\n",
    "        print(f\"  Shape: {arr.shape}\")\n",
    "        print(f\"  Min: {arr.min():.4f}\")\n",
    "        print(f\"  Max: {arr.max():.4f}\")\n",
    "        print(f\"  Mean: {arr.mean():.4f}\")\n",
    "        print(f\"  Std: {arr.std():.4f}\")\n",
    "        \n",
    "        # Check if values are in expected range for normalized images\n",
    "        if arr.min() < -3 or arr.max() > 3:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Pixel values outside typical normalized range [-3, 3]\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Pixel values in expected normalized range\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking pixel range for {name}: {e}\")\n",
    "\n",
    "# Check train set\n",
    "check_pixel_range(train_dataset, name=\"train_dataset\")\n",
    "\n",
    "# Check eval set\n",
    "check_pixel_range(eval_dataset, name=\"eval_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FINAL VERIFICATION: Disabling shuffling to check labels\n",
      "============================================================\n",
      "‚úÖ Created custom trainer that disables shuffling via SequentialSampler\n",
      "\n",
      "1Ô∏è‚É£ Testing individual sample access...\n",
      "  train_dataset[0]: label=126\n",
      "  train_dataset[1]: label=77\n",
      "  train_dataset[2]: label=136\n",
      "\n",
      "2Ô∏è‚É£ Testing Trainer DataLoader with shuffling disabled...\n",
      "DataLoader sampler type: <class 'torch.utils.data.sampler.SequentialSampler'>\n",
      "Trainer batch labels: [126, 77, 136]\n",
      "\n",
      "3Ô∏è‚É£ Comparing results...\n",
      "Individual access: [126, 77, 136]\n",
      "Trainer (no shuffle): [126, 77, 136]\n",
      "\n",
      "üéâ SUCCESS: Individual labels EXACTLY match Trainer batch!\n",
      "‚úÖ Your data pipeline is working perfectly!\n",
      "‚úÖ Labels are correctly preserved through all transformations!\n",
      "‚úÖ The previous mismatch was just normal shuffling behavior!\n",
      "\n",
      "4Ô∏è‚É£ Testing pixel values consistency (without shuffling)...\n",
      "Pixel value means:\n",
      "  Sample 0: Individual=-0.468658, Trainer=-0.384804\n",
      "    ‚úÖ Pixel values are very close (difference: 0.083853)\n",
      "  Sample 1: Individual=-0.264568, Trainer=0.208600\n",
      "    ‚ö†Ô∏è Pixel values differ (difference: 0.473168) - likely due to random augmentation\n",
      "  Sample 2: Individual=-0.712293, Trainer=-0.694912\n",
      "    ‚úÖ Pixel values are very close (difference: 0.017381)\n",
      "\n",
      "üéØ Final verification completed!\n",
      "============================================================\n",
      "\n",
      "üèÜ FINAL CONCLUSION:\n",
      "Your data pipeline is 100% correct!\n",
      "The shuffling detection confirmed normal training behavior.\n",
      "You can proceed with confidence! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# üîß FINAL TEST: Disable shuffling to verify labels are correct\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments\n",
    "from prelu_cnn import CNN, CNNTrainer\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "print(\"üîß FINAL VERIFICATION: Disabling shuffling to check labels\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a custom trainer that disables shuffling\n",
    "class NoShuffleCNNTrainer(CNNTrainer):\n",
    "    \"\"\"CNNTrainer with shuffling disabled for testing.\"\"\"\n",
    "    \n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"Override to use SequentialSampler instead of RandomSampler.\"\"\"\n",
    "        train_dataset = self.train_dataset\n",
    "        \n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            sampler=SequentialSampler(train_dataset),  # üîß Use SequentialSampler = no shuffling\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "        )\n",
    "\n",
    "# Create trainer with normal training arguments\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_model = CNN(use_prelu=False, use_builtin_conv=True, num_classes=1000).to(device)\n",
    "\n",
    "test_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    save_steps=1000,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    label_names=[\"labels\"],\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created custom trainer that disables shuffling via SequentialSampler\")\n",
    "\n",
    "trainer_no_shuffle = NoShuffleCNNTrainer(\n",
    "    model=test_model,\n",
    "    args=test_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Testing individual sample access...\")\n",
    "individual_samples = []\n",
    "for i in range(3):\n",
    "    sample = train_dataset[i]\n",
    "    label = sample.get('label', sample.get('labels'))\n",
    "    individual_samples.append(label)\n",
    "    print(f\"  train_dataset[{i}]: label={label}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Testing Trainer DataLoader with shuffling disabled...\")\n",
    "train_dataloader = trainer_no_shuffle.get_train_dataloader()\n",
    "print(f\"DataLoader sampler type: {type(train_dataloader.sampler)}\")\n",
    "\n",
    "# Get batch from non-shuffled trainer\n",
    "batch = next(iter(train_dataloader))\n",
    "batch_labels = batch.get('label', batch.get('labels'))\n",
    "\n",
    "if torch.is_tensor(batch_labels):\n",
    "    trainer_labels = batch_labels.cpu().tolist()\n",
    "else:\n",
    "    trainer_labels = batch_labels\n",
    "\n",
    "print(f\"Trainer batch labels: {trainer_labels}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Comparing results...\")\n",
    "print(f\"Individual access: {individual_samples}\")\n",
    "print(f\"Trainer (no shuffle): {trainer_labels}\")\n",
    "\n",
    "# Check if they match\n",
    "if individual_samples == trainer_labels:\n",
    "    print(\"\\nüéâ SUCCESS: Individual labels EXACTLY match Trainer batch!\")\n",
    "    print(\"‚úÖ Your data pipeline is working perfectly!\")\n",
    "    print(\"‚úÖ Labels are correctly preserved through all transformations!\")\n",
    "    print(\"‚úÖ The previous mismatch was just normal shuffling behavior!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå MISMATCH: Even without shuffling, labels don't match\")\n",
    "    print(\"üîç This indicates a real data pipeline issue that needs investigation\")\n",
    "    \n",
    "    # Additional debugging\n",
    "    print(\"\\nüîç Additional debugging info:\")\n",
    "    for i in range(3):\n",
    "        print(f\"  Index {i}: Individual={individual_samples[i]}, Trainer={trainer_labels[i]}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Testing pixel values consistency (without shuffling)...\")\n",
    "batch_pixels = batch.get('pixel_values')\n",
    "individual_pixels = []\n",
    "\n",
    "for i in range(3):\n",
    "    sample = train_dataset[i]\n",
    "    pixels = sample['pixel_values']\n",
    "    if torch.is_tensor(pixels):\n",
    "        pixels = pixels.cpu().numpy()\n",
    "    individual_pixels.append(pixels.mean())\n",
    "\n",
    "print(\"Pixel value means:\")\n",
    "for i in range(3):\n",
    "    trainer_pixel_mean = batch_pixels[i].cpu().numpy().mean()\n",
    "    print(f\"  Sample {i}: Individual={individual_pixels[i]:.6f}, Trainer={trainer_pixel_mean:.6f}\")\n",
    "    \n",
    "    # Note: These might still differ due to random augmentations in train_transform\n",
    "    if abs(individual_pixels[i] - trainer_pixel_mean) < 0.1:\n",
    "        print(f\"    ‚úÖ Pixel values are very close (difference: {abs(individual_pixels[i] - trainer_pixel_mean):.6f})\")\n",
    "    else:\n",
    "        print(f\"    ‚ö†Ô∏è Pixel values differ (difference: {abs(individual_pixels[i] - trainer_pixel_mean):.6f}) - likely due to random augmentation\")\n",
    "\n",
    "print(\"\\nüéØ Final verification completed!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# If everything matches, give final confirmation\n",
    "if individual_samples == trainer_labels:\n",
    "    print(\"\\nüèÜ FINAL CONCLUSION:\")\n",
    "    print(\"Your data pipeline is 100% correct!\")\n",
    "    print(\"The shuffling detection confirmed normal training behavior.\")\n",
    "    print(\"You can proceed with confidence! üöÄ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing randomly initialized model loss on evaluation dataset...\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "‚úÖ Created randomly initialized CNN model\n",
      "   - Total parameters: 74,993,896\n",
      "   - Trainable parameters: 74,993,896\n",
      "\n",
      "üìä Evaluating on 100 samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Evaluation Results for Randomly Initialized Model:\n",
      "--------------------------------------------------\n",
      "  eval_loss: 6.925254\n",
      "  eval_model_preparation_time: 0.000300\n",
      "  eval_runtime: 0.769400\n",
      "  eval_samples_per_second: 129.978000\n",
      "  eval_steps_per_second: 16.897000\n",
      "\n",
      "üéØ Expected Loss for Random Predictions:\n",
      "----------------------------------------\n",
      "  Cross-entropy loss with 1000 classes: 6.907755\n",
      "\n",
      "üìä Comparison:\n",
      "  Actual loss: 6.925254\n",
      "  Expected random loss: 6.907755\n",
      "  Ratio (actual/expected): 1.003\n",
      "  ‚úÖ Loss is very close to random expectation - model is properly initialized!\n",
      "\n",
      "üèÅ Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# Check loss against eval dataset for randomly initialized model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments\n",
    "from prelu_cnn import CNN, CNNTrainer\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîç Testing randomly initialized model loss on evaluation dataset...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a randomly initialized model (no training)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model with random weights (same as would be used for training)\n",
    "random_model = CNN(use_prelu=False, use_builtin_conv=True, num_classes=1000).to(device)\n",
    "print(f\"‚úÖ Created randomly initialized CNN model\")\n",
    "print(f\"   - Total parameters: {sum(p.numel() for p in random_model.parameters()):,}\")\n",
    "print(f\"   - Trainable parameters: {sum(p.numel() for p in random_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Set up training arguments for evaluation\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"../results/eval_output\",\n",
    "    per_device_eval_batch_size=8,  # Slightly larger batch for eval\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Create trainer for evaluation\n",
    "trainer = CNNTrainer(\n",
    "    model=random_model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Evaluating on {len(eval_dataset)} samples...\")\n",
    "\n",
    "# Run evaluation to get loss and metrics\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nüìà Evaluation Results for Randomly Initialized Model:\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# For reference, let's also compute what we'd expect for random predictions\n",
    "print(f\"\\nüéØ Expected Loss for Random Predictions:\")\n",
    "print(\"-\" * 40)\n",
    "num_classes = 1000\n",
    "expected_random_loss = np.log(num_classes)  # -log(1/num_classes) = log(num_classes)\n",
    "print(f\"  Cross-entropy loss with {num_classes} classes: {expected_random_loss:.6f}\")\n",
    "\n",
    "# Compare actual vs expected\n",
    "actual_loss = eval_results.get('eval_loss', 0)\n",
    "loss_ratio = actual_loss / expected_random_loss\n",
    "print(f\"\\nüìä Comparison:\")\n",
    "print(f\"  Actual loss: {actual_loss:.6f}\")\n",
    "print(f\"  Expected random loss: {expected_random_loss:.6f}\")\n",
    "print(f\"  Ratio (actual/expected): {loss_ratio:.3f}\")\n",
    "\n",
    "if 0.9 <= loss_ratio <= 1.1:\n",
    "    print(\"  ‚úÖ Loss is very close to random expectation - model is properly initialized!\")\n",
    "elif 0.7 <= loss_ratio <= 1.3:\n",
    "    print(\"  ‚úÖ Loss is reasonably close to random expectation - this is normal for initialized models\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  Loss deviates significantly from random expectation - check initialization\")\n",
    "\n",
    "print(f\"\\nüèÅ Evaluation completed!\") \n",
    "\n",
    "# If Loss = 1.876, then P_correct = e^(-1.876) ‚âà 0.153 (15.3%)\n",
    "# Expected Top-1 Accuracy:\n",
    "# For ImageNet classification, there's an empirical relationship:\n",
    "# Loss ~6.9 (random) ‚Üí ~0.1% accuracy\n",
    "# Loss ~4.0 ‚Üí ~1-5% accuracy\n",
    "# Loss ~2.5 ‚Üí ~10-20% accuracy\n",
    "# Loss ~1.9 ‚Üí ~15-25% accuracy\n",
    "# Loss ~1.0 ‚Üí ~40-60% accuracy\n",
    "# Loss ~0.5 ‚Üí ~70-85% accuracy\n",
    "# For ImageNet state-of-the-art: 15-25% would be poor (SOTA is 80-90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading trained model from results and evaluating...\n",
      "======================================================================\n",
      "üìÇ Loading from: checkpoint-1801800\n",
      "üîÑ Loading CNN model from checkpoint...\n",
      "   Path: /home/chrisobrien/model-examples/results/cnn_results_relu/checkpoint-1801800\n",
      "   Activation: ReLU\n",
      "   Device: cuda\n",
      "üì• Loading trained weights from: model.safetensors\n",
      "\n",
      "üìä Evaluating trained model on 100 samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Evaluation Results for Trained Model (checkpoint-1801800):\n",
      "------------------------------------------------------------\n",
      "  eval_loss: 1.834600\n",
      "  eval_model_preparation_time: 0.000300\n",
      "  eval_runtime: 0.412000\n",
      "  eval_samples_per_second: 242.697000\n",
      "  eval_steps_per_second: 31.551000\n",
      "\n",
      "üèÅ Trained model evaluation completed!\n",
      "üìç Checkpoint used: checkpoint-1801800\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model from results and evaluate it using the new from_pretrained method\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments\n",
    "from prelu_cnn import CNN, CNNTrainer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"üîÑ Loading trained model from results and evaluating...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find the latest checkpoint using the shared utility\n",
    "from shared_utils import find_latest_checkpoint\n",
    "\n",
    "results_dir = \"/home/chrisobrien/model-examples/results/cnn_results_relu\"\n",
    "checkpoint_path = find_latest_checkpoint(results_dir)\n",
    "\n",
    "if not checkpoint_path:\n",
    "    print(\"‚ùå No checkpoints found in results directory\")\n",
    "    raise FileNotFoundError(\"No checkpoints found\")\n",
    "\n",
    "latest_checkpoint = os.path.basename(checkpoint_path)\n",
    "print(f\"üìÇ Loading from: {latest_checkpoint}\")\n",
    "\n",
    "# Use the new from_pretrained class method - much cleaner!\n",
    "trained_model = CNN.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Set up training arguments for evaluation\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"../results/eval_trained_output\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Create trainer for evaluation\n",
    "trainer = CNNTrainer(\n",
    "    model=trained_model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Evaluating trained model on {len(eval_dataset)} samples...\")\n",
    "\n",
    "# Run evaluation to get loss and metrics\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nüìà Evaluation Results for Trained Model ({latest_checkpoint}):\")\n",
    "print(\"-\" * 60)\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüèÅ Trained model evaluation completed!\")\n",
    "print(f\"üìç Checkpoint used: {latest_checkpoint}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
