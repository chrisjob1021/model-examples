{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d77aaded234011890bd2b1841ee148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/1550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bfea7e1f8d47edb7bf2685c8cbf0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pixel_values', 'labels'],\n",
       "        num_rows: 1280967\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['pixel_values', 'labels'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prelu_cnn import CNN\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load training dataset\n",
    "dataset_path = \"../processed_datasets/imagenet_processor\"\n",
    "ds = load_from_disk(dataset_path)\n",
    "ds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 3, 224, 224]), torch.Tensor)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds['train'].select(range(1000))\n",
    "ds['pixel_values'].shape, type(ds['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ds['pixel_values']\n",
    "labels = ds['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1230,  0.6148,  0.0310,  ...,  0.0144,  0.2355,  0.3013],\n",
       "        [-0.5012,  0.5038, -0.2886,  ...,  0.2321, -0.0557, -0.4637],\n",
       "        [ 0.2678,  0.4043, -0.4985,  ...,  0.0567,  0.1876,  0.6038],\n",
       "        ...,\n",
       "        [ 0.1555,  0.4504, -0.7211,  ..., -0.4138,  0.0079,  0.3764],\n",
       "        [ 0.5574,  0.0928,  0.0194,  ..., -0.4015, -0.3036,  0.7183],\n",
       "        [ 0.0968,  0.8129,  0.6444,  ...,  0.0357,  0.3662,  0.9774]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model and inputs to GPU if available\n",
    "model = CNN(use_prelu=False, use_builtin_conv=True).to(device)\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 82 833 186 429 390 356 458 658 687 317  61 864 630  73 951 664 248 617\n",
      " 478 630 356 830 658 141  24 703 648 355  69 899 830 797 355 439 492 191\n",
      " 362 217 213 899 443 903 492 450 124 934 138 479 995 134 503 428 617 687\n",
      " 699 554 469 577 679 283 420 630 390 782 296 750 665 414  61 275 750 142\n",
      " 554 612 750 565 844 687 892 163 924  24 469 470 507 414 152 469 699 830\n",
      " 100 775 285  19 149 443 414 395 679 701 778  24 950 107 974 554 836  24\n",
      "  73 679 630 353 487 116 782 451 871 699 443 833 443 750 315 932 269 483\n",
      " 497 714 959 554 836 356 679 578 658 378 296 671 283 719 384  82 546 998\n",
      " 492 469 325 539 390 469 964 194 679 714 211  24 215 932 714 612 138 414\n",
      " 395 390 186 899 614 296 414  61 799 263 256 999 859 138 161 799  53 729\n",
      " 915 630 714 662 488 630 714 182 443 443 492 202 469 439 481 554  24  24\n",
      " 577 577 668 612 687 283 325 456 443 830 428 577 194  61 395 626 142 714\n",
      " 459 492 799 469 284 714 995  61   5 265 945 957   3 414 507 362 687  24\n",
      " 578  83 577 450  24  24 732 289  24 799 506  24 687 488 572 152 354 754\n",
      " 999  24 762 775 470 768 390 469 456 577 691 974 577 687 687 138  93 743\n",
      " 836 199 894 414  69 665 227 443 145  24 217 714  82 577 284 284 928 492\n",
      " 630 612  82 152 414 428 714 719 492  24 906  82 737 159 899 974 192 668\n",
      " 414 525 138 300 443 578  69 668 319 999 399 138  60 152 181 567 956 469\n",
      "  47 443 832 451 492 202 658 231 701 184 525 630 621 332 451 630 283 390\n",
      " 159  24  69 921 122  60 782 995 443 559 124 714 723 577 138 995 478 422\n",
      " 630 886 778 945 492 213 775 390 230  24 266 128  24 217 757 333 478 775\n",
      " 152 714 903  48 425  24 923 836  24 362 451 428 462 414 630  50  82 470\n",
      " 484 394 782 153 869  79 451 390  16 665 528 478 263 775 719 775 469   1\n",
      " 799 443 719 439 703 687 134  10 735 658 535 576 719 525 546 469 414 768\n",
      " 492 159  88 546 492 142 782 492 824 704 799 230 492 899 265 279  24 775\n",
      " 612  52 836 443 470 757 122 524 630 799  24 589 258 879 390 703 507 356\n",
      " 303 474 390 379 138 932 149 163 995 843 443 630  82 422 367 296 630 750\n",
      " 124 956 679 630 818 470 163 142 714  92 635 840 211 149 866  44 364 181\n",
      " 853 778 100 768 217 231  82 714 707 217 995 830 483 679 112 775 538 475\n",
      " 818  12  69 443 577 395 679 833 390 296   0 311 903 630 630 414 194 903\n",
      " 668 207 390  82 443 408 578 781 469 679 942 687 535 247 399 928 974 703\n",
      "  83 630 768  61 142 469 184 833 507 934 247 451 259 172 830 149 932 510\n",
      " 462 618 390 730 879 554 589 492  61 714 114 462 768 462  24 469  24 535\n",
      " 932 691 899 364  61 843 583 325 928 630 679 492 488 593 782  24 658 768\n",
      " 484 798  24 414 788 353 332 876 390 414 439 892 775  24  61 142 493 217\n",
      " 305 956 296 942 546 315 395  76 665 492 414  61 231  61 768 750 332 768\n",
      " 799  24 152 639 630 360 414 343 333 414  86 414 719 621 932 820 844 507\n",
      " 138 658 284 284 414  82 673 768 153 757 671 714 737  69 757 714 357  24\n",
      "  24 699 578 836 882 679  60 909 687 915 138 701 469 469 783 305 658 630\n",
      " 467  24 296  69 645 630  69 257 325 679 722 479 414 899  57 630  24 138\n",
      " 630 152 775 503 111 630 778  24 487 418  58 138 703 116 578 899 915 775\n",
      " 974 719 853  71 231 270 400 612 395 833 881  24 206 599 428  82 714 882\n",
      " 714 414 798 818 757 714 798 832 395 714 735 182 163 701 306 687 782  82\n",
      " 554 332 142 723  73 390  61 995  24 832  92 691 671 459 284 577 578 782\n",
      " 939 701  35 658 570 714 414 915 788 128 414 353 610 830 362 945 443 768\n",
      "  32 836 679 957 554  90 478  92 478 830  24 630 284 241 899 462 906 840\n",
      " 152 782 546  77 492   0 390 152 163 230 283 939 995 470  74 325 138 443\n",
      "  24 100 332 236  73 840 577 484  24 995 836 138 138 956 439 269  24 714\n",
      "  61 159 147 662 932 577 244 546 679 165 439 315 921 665 256 265 469 959\n",
      " 756 782 149  24 492 714 134 425 658 923  24 143 630 658 478 128  24 844\n",
      " 296 319  24 577 414 375 138  83  24 578  61 435 244 648 704 554 954 138\n",
      " 472  24  61 719 679 799 138 661 414 364 231 332 270  24  24 273 974 934\n",
      " 443  82 492 138 983 892 360 360  61 356 492 138 128 408 456 617  24  90\n",
      " 554 332 481  24  60 577 871 668 142 414 719 231 484 768 356 995  69 458\n",
      " 251 894 469 478 525 830 719 899 928  24 244 699 138  24 362   1  24 364\n",
      " 932 525 701 701 194 153  24  24 131  92]\n",
      "Labels: [726 917  13 939   6 983 655 579 702 845  69 822 575 906 752 219 192 191\n",
      " 292 848 108 372 765 473 525 639 686  99 127  76 905 550  30 634 907 979\n",
      " 718 154 914 293   9 922 130  33 968 719 653 840 139 198 236 304 547 940\n",
      " 215 853 805  28 104  67 311 429 941 950 603 971 486 504 497 670 459 559\n",
      " 940   9 829 888 773 784 782 579 714 274 146 245 761 256 326 264 827 690\n",
      " 973 130  91 615 301 361 614 572  92 303 304 799 362 222 371 547 735 121\n",
      " 165 509  79 556 821 927 969 551 375 100 524 357 584 737 716 239  41 139\n",
      " 445 484 739 942 540 462 737 998 407 491 646 607 735 599  69 955  53 514\n",
      "   6 764 918 389 854 559 927 587 344 428  45 188 411  59  28 757 511 274\n",
      " 443 715 659 609 960 775 743 580 196 546 765 279 273 230 587 535 865  62\n",
      "  27 365 226 252 914 295 664 842 211 468 733 738 921 789 973 903 504 741\n",
      " 231 624 635 483 929 133 871 702 351 293 592 521 541 601 436 249 376  19\n",
      "  81 717 733 988 270 531 210 237 918 599 979 259  40   0 422 261 363  28\n",
      " 421 171 235 908 664 192 592 846 444 705 726 747 593 206 140 865 959 168\n",
      " 441 353 357 756 669 197 168 125 234  31 424 440 455 242 619 260 492  42\n",
      " 165 299 608 887 840 716 245 385 969 360 641 749 695 351 703 139 919 809\n",
      " 167 834 513 337 805 329  87 340 132 544 748 709 517 816 526 524 210 251\n",
      " 854 945 384 241 792  96 463 897 318 876 845 964  55 345 573 408 315 573\n",
      " 191 887 642 343 658 888 352 378 106 822 191 678 129 164   4 982 319 602\n",
      " 552 389 497 341 152 678 350   2  39 104 605 243 469 804  59 387  78  84\n",
      " 722 297 271 295 682  60 678  23 835 997 344 493 957 911 776 216 323 864\n",
      " 125 257 696 760 683 787 252 323 460 267 208 533 554 682 939 298 334 130\n",
      "  46 450 835 469 702 733 744 621 797 672 454 944 773 965 234 425 770  14\n",
      "  17 965 402 516 107 310 257 590  28 446 573 369 270 224  34 661 918 698\n",
      " 855 124 862 346 189 122 691 323 761 357 665 638 754 478  18 800 706 447\n",
      " 226 756  25 588 149 507 603 110 947 158 944 652 250 924 522 969   2 922\n",
      " 999 234 588 775 532 307 305 869 455 483  30 992 910 232 109 273 502  48\n",
      " 807 290  36 527 347 963 779 472 638 149 575  30 292 973 648 324 640 976\n",
      " 380 285  57 197 807 264 414 938 397 462 937 750 780 172 319 861 994 138\n",
      " 530 176 451 891 741 495 656 230  98 173 869 325 520 488 278 650 381 281\n",
      " 648 400 760 230 544 298 523 882 153 540 744 805 457 163 813 186 758 726\n",
      " 353 906 997 257 156 966 533 763 555 379 578 378 873   6 304 694 635 555\n",
      " 544 805 712 522 888 849 584 209 409 276 101  44 936 521 382 360 567 237\n",
      " 341 587 890 304 911 152 799 569  19 173 138 408 516 995 423 225 452  71\n",
      " 317  74 891 421 461 909 913 592 731 139 349 486 815 973 975 670 475 355\n",
      " 275 448 224  66 192 295 242  93 261 370 526 947 570 972 938  14 731 429\n",
      " 860 536 176 834 555 674 450 229 237 922 366 777  74 401 211  73 851 811\n",
      " 910 952 493 861 819 197  86 760 495 956  11 368  62 658 913 100 682 928\n",
      " 189 758 187 540 301 535 808 450 807 114 674 757 206 831 516 812 793 501\n",
      " 411 691 509 642 802   9 358 442 593  35 665 988 594  13 563 845 103 766\n",
      " 293 888 233 590 843   6 592 248 581 620 579 146 208 566 628 450 503 779\n",
      " 937 200 420 699 651 220 246 174 278 113 445 601 655 970 506 355 705 440\n",
      " 260 822 971 686  81 808 506 902  11 658 556 118 344 324 857 115  67 733\n",
      " 583 417 508 347 546 724 351 312 463 402  49 359 419 797 850 312 988 143\n",
      " 436 523 107 321 461 177 466 437 162 860 270 448  71 356 838 678  27 947\n",
      " 172 447 438 890   2 745 210 181 871 561 280 697 110 985 545  20 837 996\n",
      " 389 788 898 563  29 215 153 533 973 415 519 207  16 674 678 396 399  14\n",
      " 884 642 441 321 479 897 627 508 381 822 140  74 638 337 236 796 245 184\n",
      " 510 924 486 367 794 338  12 359 674 197 704 621 382 916 312  97 990 605\n",
      " 752 867 952 734 737 932 667 201 685 133 657 334 936 525  96 232 912 623\n",
      " 558  84 563 806 284 721 711 481 550 891 179 508 194 803 865 707 673 294\n",
      " 157 206 102 838 269 898 322 926 987 939 985 884 216 371 897 360 687 312\n",
      " 899 829 154 928 623 449 290 662 281 879 620 724 814 833 491 463 522 837\n",
      " 578 325 331 908 235 316  19 153 942 799 877 540 733   3 376 641 827 359\n",
      " 536 417  91 599 645 781 876 245   0 192  73 595 848 228  90  54 916  11\n",
      " 451 930 476 699 781 417 452 659 661  97]\n",
      "Correct predictions: 0 out of 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# axis=1 means we're finding the maximum value along the second dimension (columns)\n",
    "# For a 2D tensor with shape (batch_size, num_classes), axis=1 gives us the class index\n",
    "# with the highest probability for each sample in the batch\n",
    "predictions = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Labels:\", labels.cpu().numpy())\n",
    "print(\"Correct predictions:\", (predictions == labels.cpu().numpy()).sum(), \"out of\", len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9875, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss = loss_fct(outputs, labels)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
