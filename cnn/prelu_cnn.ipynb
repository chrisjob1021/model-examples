{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prelu_cnn import CNN\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load training dataset\n",
    "dataset_path = \"../processed_datasets/imagenet_processor\"\n",
    "ds = load_from_disk(dataset_path)\n",
    "ds = ds['train']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 3, 224, 224])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ds['pixel_values']\n",
    "labels = ds['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0947, -0.0165,  0.0874,  ..., -0.1580, -0.0219, -0.2634],\n",
       "        [ 0.1340,  0.1958,  0.2236,  ..., -0.3096,  0.0041, -0.0962],\n",
       "        [-0.0111, -0.2845,  0.1285,  ..., -0.2256,  0.1135, -0.1852],\n",
       "        ...,\n",
       "        [-0.4496, -0.3307,  0.1620,  ..., -0.1684,  0.2932, -0.5237],\n",
       "        [-0.1339,  0.2442, -0.1870,  ..., -0.4475,  0.1409, -0.2245],\n",
       "        [ 0.2570,  0.3836,  0.0375,  ..., -0.0107,  0.1787, -0.0538]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model and inputs to GPU if available\n",
    "model = CNN(use_prelu=False, use_builtin_conv=True).to(device)\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [455 837 432 354 640 554 867 259 464 929 752  32 121 152 694 152 442 317\n",
      " 317 183 664 102 656 477   9 731 121 431 714 837 725 837 446 442   9 431\n",
      " 694 853 583 534 899 294 229 837 275 317 752 497 941 250 779 478 242 394\n",
      " 771 114 446 958  50 313 822 450 358 314 540 846 744 480   9 183   9 530\n",
      " 384   9  32   9 317 313   9 510 317 878 480 889  42 313  80 867 609 672\n",
      " 480 656 182  32 478 121 664 664 779 664 116 478   9 664 976 846 384 431\n",
      "   2 247 622  32 837 143 878 741 314 741 767 114 317 975 699 352  32  42\n",
      "   9   9 250 817 431 837 804  23 865 183 152 725 976 744  42 528 362 484\n",
      "  18  32 837  23 102 446 110  78 368   9 867   9 450 744   9 440 959 846\n",
      " 431 589 364 744 317 845 653 744 102 838  73 294 200 798 547 418  52 464\n",
      "  43 622 313   9 583 959 687 313 314 440   9 998 867 313 431 317 317  22\n",
      " 622 317 560  78 478 837 694 125 469 837 442 250 846   9 501 387 878  32\n",
      " 362 765 859 431 789 554  42 478 425 719 310 317 810 850   9 431  78 317\n",
      "   9 656 612 556 294 725  52 294 327 214 548   9 822   9 133 810 275   9\n",
      " 689 656 725 469 963 857 848 873 530 779 102 200 765 664 847   9 524 976\n",
      " 367 317 741 110 779 765 998  78 959 388  63 747 712 837 239 725 829 357\n",
      " 694   9 838 656 183 362 959 313 121 542 317 314 202 317 393 114   9 656\n",
      " 102 771 932 110 907 704 393   9 725 731 313 294 388 767 534 725 317 173\n",
      " 245 694 853 626 294 960 314 396 313   9 153 313 976  55 756 682 431  78\n",
      " 478 987 313  42 807 511 183 699 719 714 771 313 920 195 294 817 200 530\n",
      " 817   9 901 172 242 294 622 744 317 364 321 334 290 317 744 829 958 275\n",
      " 731 431 477 391 317 114  52 123 811 250 340 781 314 480 215 899   9 317\n",
      " 349 294  43 183   9  33 878 125 554 211 838   9 719 976 546   9   9 664\n",
      " 431 480 694 250  22 200 202 317 393 102 347  74   9 689 664 530 313 885\n",
      " 309 670 779 396 418 446 102 601 265 116 687 462 123 167   9 478  20 664\n",
      " 114 780 765 768 542 313 391 837 950 514 367 741   9   9 547 506 250 250\n",
      "   9 431 460   9 913 362  63 664 391   9 744 853  23 694   9 317 848 622\n",
      " 654 317 771 114   9 313 250   9 725 516 133 313 388  78   9 317 664 896\n",
      " 114 387 317 454 665 747  32 765 725 176  20   9 340 589 765 317 313 123\n",
      "  32 555 446 976 765 317 542 480 719 317 114   9 440 643 317 102 555 743\n",
      "   9 444  83 845 920 391   9 294 765 640   9  32 656 798 765 764 121 431\n",
      " 183 183 889 183 638 867 817 317   9  78 867 115  78 317 152 779 313 878\n",
      " 622 317 387 626   9 610  31 525  47 929 393  32 822 829 822 196 431 626\n",
      " 393 288 183 313 313 466  47 102 458 313 776 741 478 863 317  32 340 857\n",
      " 296 317 945 821 725 317  63 317 807 183 182 821   9   9 321 294 932 469\n",
      " 317 123 321 837 294   9 530 562 554   9 609 511 530 853 275 959 570 314\n",
      " 592 303 313 708 123  42 316 626 625 664   9 258 431 896 694 664   9  83\n",
      " 854 478 358   9 314 536 313 846  52 394 889 656 848 530   9 863 941 469\n",
      " 123 810 857 184 846 817 294 664   9 480 283 115  33 364  78 317 981  32\n",
      " 857 554 771 200 946 710 316  32 846 577 219   9 316 625 168 317 313   9\n",
      " 656 848 250 837 845 725 340 183 593 317 837 114 316   9 744 431 182 414\n",
      " 601  78 656 275 310 622 313 976 656  32 846 202 314 480 364 725 506 725\n",
      " 976   9 296 340  31 664 442 946  23 296 544 744 293 478 589 378 272 837\n",
      "   9 464  68  32 845 387 446  47  32 907 712 725 951 907   9 546 321 313\n",
      "   9 391 478 315 779   9 845 294 779 779 976 387 779 431 114 114   9 609\n",
      "   9 846 779 959 478 725 275 102 725 387 478 694 656 926 289 664  78  47\n",
      " 745 455 596 421 771 765 977 960 313 480 867 478 294 789 676 431 656 421\n",
      " 183  42 183 480 440 939   9 391 960 377 262  32 152 610 377 650  23 771\n",
      " 878 664   9 446 610 932 317 317 706 480 846 200 121  78 589 960 316   9\n",
      " 313   9 837 480 817 421  52 272 913 941 976 929 455 313  74 725 250 431\n",
      "  15 752 313 524 391 313   9 441 976 664 184 313 964 200   9 317 972 958\n",
      " 745 288 480  47  78 583 676 641 570 656   9 314 907 554 857 351   9   9\n",
      "  23 703 183 765 154 396 268  32 431 530   9   9   9  32 542 366 377 804\n",
      " 317 318 704 694 391 704  47   9 548 594 123   9 186 804 313 846  23 477\n",
      " 664 293  69  42 779  98   9 656 853   9 314 554   9 478 478 822 530  78\n",
      " 779 377 431 913 838 313   9 626   9 116]\n",
      "Labels: [726 917  13 939   6 983 655 579 702 845  69 822 575 906 752 219 192 191\n",
      " 292 848 108 372 765 473 525 639 686  99 127  76 905 550  30 634 907 979\n",
      " 718 154 914 293   9 922 130  33 968 719 653 840 139 198 236 304 547 940\n",
      " 215 853 805  28 104  67 311 429 941 950 603 971 486 504 497 670 459 559\n",
      " 940   9 829 888 773 784 782 579 714 274 146 245 761 256 326 264 827 690\n",
      " 973 130  91 615 301 361 614 572  92 303 304 799 362 222 371 547 735 121\n",
      " 165 509  79 556 821 927 969 551 375 100 524 357 584 737 716 239  41 139\n",
      " 445 484 739 942 540 462 737 998 407 491 646 607 735 599  69 955  53 514\n",
      "   6 764 918 389 854 559 927 587 344 428  45 188 411  59  28 757 511 274\n",
      " 443 715 659 609 960 775 743 580 196 546 765 279 273 230 587 535 865  62\n",
      "  27 365 226 252 914 295 664 842 211 468 733 738 921 789 973 903 504 741\n",
      " 231 624 635 483 929 133 871 702 351 293 592 521 541 601 436 249 376  19\n",
      "  81 717 733 988 270 531 210 237 918 599 979 259  40   0 422 261 363  28\n",
      " 421 171 235 908 664 192 592 846 444 705 726 747 593 206 140 865 959 168\n",
      " 441 353 357 756 669 197 168 125 234  31 424 440 455 242 619 260 492  42\n",
      " 165 299 608 887 840 716 245 385 969 360 641 749 695 351 703 139 919 809\n",
      " 167 834 513 337 805 329  87 340 132 544 748 709 517 816 526 524 210 251\n",
      " 854 945 384 241 792  96 463 897 318 876 845 964  55 345 573 408 315 573\n",
      " 191 887 642 343 658 888 352 378 106 822 191 678 129 164   4 982 319 602\n",
      " 552 389 497 341 152 678 350   2  39 104 605 243 469 804  59 387  78  84\n",
      " 722 297 271 295 682  60 678  23 835 997 344 493 957 911 776 216 323 864\n",
      " 125 257 696 760 683 787 252 323 460 267 208 533 554 682 939 298 334 130\n",
      "  46 450 835 469 702 733 744 621 797 672 454 944 773 965 234 425 770  14\n",
      "  17 965 402 516 107 310 257 590  28 446 573 369 270 224  34 661 918 698\n",
      " 855 124 862 346 189 122 691 323 761 357 665 638 754 478  18 800 706 447\n",
      " 226 756  25 588 149 507 603 110 947 158 944 652 250 924 522 969   2 922\n",
      " 999 234 588 775 532 307 305 869 455 483  30 992 910 232 109 273 502  48\n",
      " 807 290  36 527 347 963 779 472 638 149 575  30 292 973 648 324 640 976\n",
      " 380 285  57 197 807 264 414 938 397 462 937 750 780 172 319 861 994 138\n",
      " 530 176 451 891 741 495 656 230  98 173 869 325 520 488 278 650 381 281\n",
      " 648 400 760 230 544 298 523 882 153 540 744 805 457 163 813 186 758 726\n",
      " 353 906 997 257 156 966 533 763 555 379 578 378 873   6 304 694 635 555\n",
      " 544 805 712 522 888 849 584 209 409 276 101  44 936 521 382 360 567 237\n",
      " 341 587 890 304 911 152 799 569  19 173 138 408 516 995 423 225 452  71\n",
      " 317  74 891 421 461 909 913 592 731 139 349 486 815 973 975 670 475 355\n",
      " 275 448 224  66 192 295 242  93 261 370 526 947 570 972 938  14 731 429\n",
      " 860 536 176 834 555 674 450 229 237 922 366 777  74 401 211  73 851 811\n",
      " 910 952 493 861 819 197  86 760 495 956  11 368  62 658 913 100 682 928\n",
      " 189 758 187 540 301 535 808 450 807 114 674 757 206 831 516 812 793 501\n",
      " 411 691 509 642 802   9 358 442 593  35 665 988 594  13 563 845 103 766\n",
      " 293 888 233 590 843   6 592 248 581 620 579 146 208 566 628 450 503 779\n",
      " 937 200 420 699 651 220 246 174 278 113 445 601 655 970 506 355 705 440\n",
      " 260 822 971 686  81 808 506 902  11 658 556 118 344 324 857 115  67 733\n",
      " 583 417 508 347 546 724 351 312 463 402  49 359 419 797 850 312 988 143\n",
      " 436 523 107 321 461 177 466 437 162 860 270 448  71 356 838 678  27 947\n",
      " 172 447 438 890   2 745 210 181 871 561 280 697 110 985 545  20 837 996\n",
      " 389 788 898 563  29 215 153 533 973 415 519 207  16 674 678 396 399  14\n",
      " 884 642 441 321 479 897 627 508 381 822 140  74 638 337 236 796 245 184\n",
      " 510 924 486 367 794 338  12 359 674 197 704 621 382 916 312  97 990 605\n",
      " 752 867 952 734 737 932 667 201 685 133 657 334 936 525  96 232 912 623\n",
      " 558  84 563 806 284 721 711 481 550 891 179 508 194 803 865 707 673 294\n",
      " 157 206 102 838 269 898 322 926 987 939 985 884 216 371 897 360 687 312\n",
      " 899 829 154 928 623 449 290 662 281 879 620 724 814 833 491 463 522 837\n",
      " 578 325 331 908 235 316  19 153 942 799 877 540 733   3 376 641 827 359\n",
      " 536 417  91 599 645 781 876 245   0 192  73 595 848 228  90  54 916  11\n",
      " 451 930 476 699 781 417 452 659 661  97]\n",
      "Correct predictions: 1 out of 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# axis=1 means we're finding the maximum value along the second dimension (columns)\n",
    "# For a 2D tensor with shape (batch_size, num_classes), axis=1 gives us the class index\n",
    "# with the highest probability for each sample in the batch\n",
    "predictions = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Labels:\", labels.cpu().numpy())\n",
    "print(\"Correct predictions:\", (predictions == labels.cpu().numpy()).sum(), \"out of\", len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9320, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss = loss_fct(outputs, labels)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
