{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd43f54edf64d7884c18ce409db814b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/1550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b216e65f5c8c4b3f803667bd554dedac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pixel_values', 'labels'],\n",
       "        num_rows: 1280967\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['pixel_values', 'labels'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prelu_cnn import CNN\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load training dataset\n",
    "dataset_path = \"../processed_datasets/imagenet_processor\"\n",
    "ds = load_from_disk(dataset_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pixel_values', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds['validation'].select(range(1000))\n",
    "# ds['pixel_values'].shape, type(ds['pixel_values'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ds['pixel_values']\n",
    "labels = ds['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2276,  0.3220, -0.3458,  ..., -0.3076,  0.0704, -0.2895],\n",
       "        [ 0.2216, -0.0497, -0.1900,  ..., -0.2768, -0.0554, -0.1468],\n",
       "        [ 0.1475,  0.2722, -0.2436,  ...,  0.1617, -0.1047, -0.3831],\n",
       "        ...,\n",
       "        [ 0.1871,  0.1047, -0.1447,  ...,  0.0891, -0.1536,  0.0940],\n",
       "        [ 0.6746,  0.2782, -0.8391,  ...,  0.5544, -0.5694, -0.6035],\n",
       "        [-0.0197,  0.0915, -0.3428,  ..., -0.1248,  0.0804,  0.0300]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model and inputs to GPU if available\n",
    "model = CNN(use_prelu=False, use_builtin_conv=True).to(device)\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [  4 610 222 197 950 844 547 904 577 812 211 415 751 308 239 506 222  20\n",
      " 560 266 427 222 309 633 222 931 344 308 812 802 221 487 885 751 638 406\n",
      " 349 885   8 736 802 451 432 704 801  60 456 222 870 432 277 904 954 598\n",
      " 577 822 816 972 577 766 183 880 734 748 638  60 183 752 644 221 649 867\n",
      " 447 547 230 730 221 263 349 885 581  45 576 444 334 126 867 553 730 173\n",
      " 230 481 875 583 805 700 333 751 931 752 359 668 428 576 977 638 576 828\n",
      " 663 631 427 239 577  17 415 153 734 744 547   4 553 486 610 994 200 875\n",
      "  89 173 447 598 875  88 239   4 343 846 802  94 867 406 154 875 658 444\n",
      " 140 176 820 994 816 603 810 577 904 481 717 173 216 820  11 342 153 400\n",
      " 880 610 802 545 658 900 183 181 870 429 820 807 631 222 547 332 308  23\n",
      " 463 176 222 900 530 788 730 697 795 970 415 222 220  91 668 222 406 385\n",
      " 197 487 465  60 788 456 560 994 664 778 447 863 165 456 589 368 885 576\n",
      "  42 432 734 618 802 553 221 222 176 154 820 944 456 751 695 663 362  86\n",
      " 950 222 994 870  94 611 782 900 717 456  70 126 752 704 801 659 456 994\n",
      "  58 638 950 222 577 384 480 963 817 263 777  91 802 638 697 120 660 931\n",
      "  42 730 684 680 820 658 977 156  10 931  39 665 197 744 547 222 156 802\n",
      " 101 812 933 649 406 332   4 427 230 821 867 651 344 321 598 239 332 994\n",
      " 362 909 167 577 788 628 467 870 409 885 905 447 788 704 183 904 379  49\n",
      " 577 802 435 931 320 660 660 843 435 386 386 950 219 195  60 863 308 165\n",
      " 967 822 199 827 663 952 764 443 332 394 463 900 197 518 774 712 631 948\n",
      " 970 549 523 775 415 456 429   6 283 168 594 928 309 972 885 154 438 680\n",
      " 786 435 156 663 638  17  19 480  60 786 975 802 744 487 885 211 751 463\n",
      " 456 308 266 801 271 905 197 867 547 435 222 820 349 318  46   4 456 992\n",
      " 239 668 456 723 988 197 687 447 813 599 392 658 518 385 392 239 900 120\n",
      " 697 156 730 239 885 638 663 254 272 197 734 867 456 553 221 875 782 752\n",
      " 119 547 805 862 959 423 549 806 684 849 126 333 668 875 154 349  60 734\n",
      " 411 481 736  96 866 658 730 126 576 186 277 782 456 379 183 577 880 864\n",
      " 415 506 812  49   4 863 506 154 521 176 332 744  60 947 320 773 447 222\n",
      " 518  59 222 221 200 697 692 332 638 394 394 432 875  39 482 663  85 362\n",
      " 193 774 379 154 239 221 222  23 167 594 730 712 222 506 560 787 183 788\n",
      " 222 463 199 936 451 649 256 751 802  56 674 660 875 463 866 875 802 994\n",
      " 738 342 394 898 577 154 435 549   4 272 950 456 645 577 663 222 516 456\n",
      " 368 904 222  70 438 611 267 880 786 730 577 390 594 406 802 977  11 577\n",
      " 769 880 488 580 120 308 751 350 663 547 885 802   4 752 576 531 413 501\n",
      " 992 173 443 977 572 576 751 197 970 342 110 805 342 764 770 425 211 674\n",
      " 197 119 634 239 221 368 332 795 443 730 320  49 987  49 384 779 734 447\n",
      " 752 852 802 222 222 272 406 802 343 447 762 308 583 172 451 970 710  11\n",
      " 394 734 385 663 221 744 394 690 599 379 221 283  42 577 770 400 668 553\n",
      " 176 957 577 663 493 173 875 755 386 658 663 147 786 452 835 332 451 200\n",
      " 648 406 852 847 931 977 996 594 518 432 154 769   0 386 199 547 806  60\n",
      " 547 800 154 977   4 153 481 875 577 406 197 454 580 506 994 126 518 456\n",
      " 695 994  85 318 138 802 463 211 456 481 211 411 717 801 870   4 576 154\n",
      " 994 384 804 153 332  86  60 386 788 343 576 717 994 631   4 211 802 200\n",
      " 500 547 734  67 866 658  47 683 154 320 308 633 918  80 577 751 349 671\n",
      "  60 343 154 751 415  42 859 314 126 379 222 239 970 219 752 658 447 880\n",
      " 770   8 456 222 487 432 994   5 699   4 712 885  49 994 342 977 308 553\n",
      " 547 649 443 447 487 802  96 905 222 734 801 668 659 751 300 730 648 752\n",
      " 582  86  60 394 589 788 498 663 240 384 633 518 211 197 994  86 154 173\n",
      " 928 690 802 342 816 610 332 183 406 577 429 875 222 386 917 895 751 730\n",
      " 423 222 263 734 309 349 432 970 429 120 805 762 404 165 308 500 196 209\n",
      " 697 954 143 751 432 576 628 443 751 308 864 154 751  86 432 900 977 386\n",
      "   0 704 752 804 494 257 308 386 362 415 222 430 344 332 197 658 553   0\n",
      " 456 549 154 980 114 359 704 964 406 959 394 427 898 936 332 432 518 800\n",
      " 239 308 283 770 885 734 970 577 386 977 788 197 394 684 977   4 994 843\n",
      " 994 447 394 553 545 936  11 423 465 880 583 530 862 712 221 126  47 820\n",
      " 862 447 239 820 594 900 802 628 628 885]\n",
      "Labels: [ 91 171 980 218  19 658 549 808 305 416  89 793 712 505 332 575 986 867\n",
      " 116 817 642 327 995  87 965 267 977 989 120 800 905 124 541 661 112 531\n",
      " 251 495 313 784 261 973 725 535 691  70 269 438 558 830 569 129  46  60\n",
      " 766 191 553 586 119 422 334  13 542 190 290 495 826 556 664 584 521 828\n",
      " 593 373  46 409 692 332 825 990 718 717 195 340 627 138 835 743 623 998\n",
      " 322 564 570 905 875 415 764 811 730 238 943  83 995 226 806 356  40 543\n",
      " 993 575 828 883 633 921 515 970 568 597  98  62 910 814 988 155 614 561\n",
      " 231 215 966 311 326  55 943 364 988 187 571 575 671  70 110 808 490 798\n",
      " 961 266 184 419 725  63 971 473  15 832 512 222 106 223 825 641 684 102\n",
      " 214 726 954 181 813 622 863 665 990 832 242 594  86 871 223 874 213 315\n",
      " 417 804 126 839 928 998  50  16 990 988 771 210 966  68 483 376 580 222\n",
      " 862 830 895 237 269  96 984 403 764  56 273 779 141 187 546 774 165 899\n",
      " 689 886 634 860 392 914 338 619 711 463 751  72 311 449 838 450 399 907\n",
      " 234 392  71 249 455 499 938 641  83 704 785 582 115 157 154 590 875 914\n",
      " 761 395 572 413 837 482 228 683 941 650 971 413  88 488 195 550 711  44\n",
      " 242 554 698 646 292 888 161  66 136 191  88 357 424 821 758 194 673 828\n",
      " 703 541 999 216 926   2  33  61 699 451 756 837  96 628 530 982 732 370\n",
      " 334 879   0 267 708 935 264 292 573 861  91 179 965 270  29 850 608   4\n",
      " 220 113 424 405 116 222 159 447 809 237 975 104  13 897 280 388 617 538\n",
      "  55 780 888 834 216 720 180 238 164 485 640 481 492 422 173 657 941 907\n",
      " 215 686 680  16 210 700 616 728 509 738 252 595   4 808 809 782 645   9\n",
      " 421 699 514 228 520 997 565 180 302 319 707 293 809 464 467 379  37 187\n",
      " 202 410 301 718 956 558 833 770 795  48 565 110 927 740 250 549 844 772\n",
      " 385 605 487 912 834 755 979 220 869 819 996 947 176 413 873 964 189 889\n",
      " 181 546 451 136 973 273 821  81 298 999 257  68 649 513 683 902 406 992\n",
      " 417 325  29 555 103 994 350 934 998 402  51 486 113 789 437 878 927 101\n",
      " 314 440 143 672 577 561 448 798 883 336 489 790 298 692 923  31 690 229\n",
      " 165 594 976 815 204 589 220 364 930 285 111 242 245 262 996 604 539 951\n",
      " 365 880 640 340 887 145 983 820 156 211 969 773 136 875 990 765 253  76\n",
      " 184 941 371 380 534  94 266 666 397 131 194 415 808 619 417 125 682 559\n",
      " 596 777 895 566 316 932 102 165 600 280 397  74 933 429 351  43 752 222\n",
      " 482  64  27 820 855 283 885 135 839 666 622 417 681 179 416  66 522 733\n",
      " 374 234 175 683 479 391 998 494 352 244 976  30 406 221  16 546 255 611\n",
      " 564 442 721 738 105 914 454 357  75 381 940 474 424 842 809 597 881 947\n",
      " 524 111 977 146 564 401 629 386 960 945 451  96  38 559 787  87 254 549\n",
      " 957   2 360 375 461 784 992 194 244 384 728 908 918 875 289 960 303 277\n",
      " 568 889 470 129 153 240 437 209 363 633 605 541 631 749 552 861 672 438\n",
      " 555 997 946 200 950 378 388 616 579 297  73 507 667 630 100 686 212 510\n",
      "  83 536 587 516 243 444 354 286 272 512 650 529 780 933 762 494  16 577\n",
      "  31 372 517 123 532 805 709 177 411 372 348 474  87 833 234 342 126 618\n",
      " 469 156 949 166 783 305 187 466 421 662  41 867 206 204 928 377 131 426\n",
      " 661 564 472 596 348 440  88 450 606 882 436 805  52 867 741  44 623 674\n",
      " 516 953 373 595 962 920 797 168 350 294 879 979 566 106 208 766 823 284\n",
      " 915 843 920 269 602 145  54 338 290 517 121 921 451 817 932 347 634 124\n",
      "  34 560 581 410 270  71 515 580 270 847 994 493 278 957  80 190 896  54\n",
      " 347 239 526 190  74 723 953 451 648 802 165 301   3 383 221   1 300  29\n",
      " 565 356 556 700 596 472 142  35 652 935 495 942 444 566 189 792 471  49\n",
      " 562 512 358 322 239 891 745 905 654 775 756 613 488 822 581 508 288 980\n",
      " 142 168 424 577 465 539 330 572 204 866 548 554 550 421 880  24  84 424\n",
      " 601 115  64 353 281 851 677 481 790 354 850 681 960 745 725 773 503 513\n",
      " 561 747  49 135 433 512 188 301 568 945 822 180 653 791 494 818 475 805\n",
      " 243 620 550 352 311 415 790 289  33 904 895 690 593 322 755 945 585 669\n",
      " 833 908 892 669 848 851 758 865 383  94 626 262 205 920 626 948 923 128\n",
      " 953 953 744 626 535 148 999 694 700 810  24 244 375 169 388 295 904 992\n",
      " 465 229 583 804 428 415 978 606 744  60 993 698 163 394 931 603 994 564\n",
      " 174 839 209 159 362 507 167 148 311 805]\n",
      "Correct predictions: 0 out of 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# axis=1 means we're finding the maximum value along the second dimension (columns)\n",
    "# For a 2D tensor with shape (batch_size, num_classes), axis=1 gives us the class index\n",
    "# with the highest probability for each sample in the batch\n",
    "predictions = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Labels:\", labels.cpu().numpy())\n",
    "print(\"Correct predictions:\", (predictions == labels.cpu().numpy()).sum(), \"out of\", len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9531, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss = loss_fct(outputs, labels)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
