{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0ade6b84c2484b99aef848f9078d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from prelu_cnn import CNN\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load training dataset\n",
    "dataset_path = \"../processed_datasets/imagenet_processor\"\n",
    "ds = load_from_disk(dataset_path)\n",
    "ds = ds['train'].select(range(1000))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 3, 224, 224]), torch.Tensor)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['pixel_values'].shape, type(ds['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ds['pixel_values']\n",
    "labels = ds['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1992, -0.0423,  0.0543,  ...,  0.1351,  0.0930, -0.2633],\n",
       "        [ 0.9272,  0.3014,  0.3716,  ...,  0.1198,  0.0743, -0.0290],\n",
       "        [-0.1074,  0.0388,  0.1548,  ...,  0.4969, -0.4893,  0.4166],\n",
       "        ...,\n",
       "        [ 0.4961,  0.1919,  0.6859,  ..., -0.3899, -0.2643,  0.3357],\n",
       "        [ 0.2203, -0.2539, -0.1453,  ...,  0.4844,  0.2236, -0.0192],\n",
       "        [-0.1353,  0.0568,  0.1968,  ...,  0.1761, -0.0790,  0.5020]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model and inputs to GPU if available\n",
    "model = CNN(use_prelu=False, use_builtin_conv=True).to(device)\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [426 186 596 596 417 815 528 603 799  71 874 320 444 857 417 945  71 333\n",
      " 298 895 974 609 525 134 246 760 241 136 926 475 188 709 763  71 510 759\n",
      " 709 892 456 188 804 759  71 926 282  22  71 601 778 484  30 282 513 929\n",
      " 458 164  22 943 881 329 505 804 321  27 393 408 488 804 282  72 298 596\n",
      " 436 645 596 228 596 303 902 748 536 988 380 428 332 580 444  71 878 473\n",
      " 293  64 645 530 530 645 892 845 596 329 528 702 337 853 829  71 809 333\n",
      " 128 507 702 596 337 596 298 955 846 767 596 596 870 265  59 163 279 134\n",
      " 246 870 898 444 899 365 384 965  71 343 671  71 134  64  71 417 702 436\n",
      " 566 890 804 804 496 945 596 417 346 699 943  54 666 945  48  21 926 596\n",
      " 266 528 738 346 174 709 164 381  35 382 701 128 340 338 589 702 512 417\n",
      " 517 382 473  41 302 703 857  71 332 926 870 545 943 596 293 945 296 473\n",
      " 188 867 382 326 436 189 365 702 444  71 403 572 783 597 478 596 282 804\n",
      " 701 964 865 550 892 213  58 545  64 673 721 943 338 401 128 837 709 596\n",
      " 709 596 526 627 654 596  58 362 945  71 748 473 662 860 393  71 943 701\n",
      " 701 405  20 320 293 525 701  71 580 701 898 248 487 846 709 686 329 660\n",
      " 502 454  71 649 293 892 248 763 417 797 596 736 926 709 596 266 804 794\n",
      " 405  71  71 567 881  22 473 164 865 188 596  27 686 926  71 715 456 365\n",
      "  20 599 279 911 645 294 951 596 293 662 609 436 164  71 701 444 282 701\n",
      " 729 701 638 596 902 931 709  82 478 596 444 332 346 898 686 337 240 487\n",
      " 134 778 282 199 108 279 945 701 332 263 213 337 662 735 638 892 609 164\n",
      " 437 804 809 296  72 299 926 472 134 413  22 809 545 596 950 408 173  95\n",
      "   3  70 662 223 163 266 381 589  41 701  22 702 444 926  22 293  71 298\n",
      " 441 340 296 580 202 572 802 144 596 702 702 645 799 845 748 857 654 702\n",
      " 645  71 252 926 326 926 865 701 759 638 458 333 627 945 512 715 888 926\n",
      "  39 444 596 545 842 709 945 223 437 503 530   7 291 282 346 596 837 943\n",
      " 362 701  72 757 228 709  22  71  90 441 715 397 396 596 809 662 645  39\n",
      " 945 943  71 329 174 865  71 848 737 444 596  71  24  30 298  21 393  71\n",
      " 943  71 762 160 890 291 853  81 528  71 444 296 405 291 469 580 393  71\n",
      " 846 520 320  71 580 296 686 709 478  54 221 926 384 596 816 660 338 713\n",
      " 974 487 716 164 110 945   3 926 393 926 282 291 246 926 701 402 702 596\n",
      " 340 773 296 870 596 393 846   7 748 473 298  22 701 479 596 157 441 188\n",
      " 701 898 460 873 293 505 286 596 595 365 340  44 904 528 473 329 399 947\n",
      " 865 292 580  72 596 895 911 505 801 804 596 780 436 686 397   7 846 164\n",
      " 892 801 759 526 510 346 856 320 898 759  39 870 566 865 416 382 888 580\n",
      " 596 801 583 321 929 702  71 388 943  89 701 282 766 701 244 945 338 888\n",
      " 596 213 417 333 282 248  22  71 417 883 702 282 846  21  70 393  71 926\n",
      " 340 926 567  22 148 945 291   2 596 945 399  98 748 596 709 393 943 275\n",
      "   4 473 895 337 282 709 857 282 702 290 188  71 382 702 397 456  31 396\n",
      " 501 759 701  72 729 333 802 551 719 376 895 926 943 417 444 545  22 320\n",
      " 551 333 174 702 926 766 335  31 830 821 721 488 832 804 436  22 329 701\n",
      " 596 174 596 748 596  71 898 837 804 596 703 534  80  53 444 417 291 550\n",
      " 344 164 701 596 988 748 580 596 692 213 436 436 572 748 231 596 189 320\n",
      " 731 487 701 596  21 599 212 171  22 444  71 293 528 837 396  72 512 436\n",
      " 329 926 554 759 701 692 701 804 799 429 402 895 260 586 701 298 282 331\n",
      " 857 397 596 964 702 702 188 437 458 759 895 773 199 380 701 282 596 110\n",
      " 701 444 464 563 382  95 799 393 821  64 702 692 596 393 991  22  71 382\n",
      "  70  68 164 926 720 393 884 370 329 804 380 282 757  24 709 811 413  72\n",
      " 458  72 892 337 298 580  58  81  71 128 521 701 635  95  72 945 266 533\n",
      " 545 396 436 596  72 662 223 748  71 282 550   4 393 291 837 417 396 645\n",
      " 729 380 786 596 735 901  57 926 405 593 701 525 249 603 894 329 436  22\n",
      "  71 473 310 709 757 238 709 738 282 759 444 808 436  37 393 603 596 709\n",
      " 686 763 473 596 586 984 395 512 804 484 386 702 528 596 903 528  71 296\n",
      " 926 478  72 171 478 473 596 260 701 567 525 300 899 596  24 801 701 974\n",
      " 801 821 832 507 748 478 456 701 892 362  24 662 393 335 395 294  24 144\n",
      " 441 723  21 759 163 754 804 394 361 164 230 384 801  71  15 337 411 528\n",
      "  21 702 530 464 458 428 993 759  71 583]\n",
      "Labels: [726 917  13 939   6 983 655 579 702 845  69 822 575 906 752 219 192 191\n",
      " 292 848 108 372 765 473 525 639 686  99 127  76 905 550  30 634 907 979\n",
      " 718 154 914 293   9 922 130  33 968 719 653 840 139 198 236 304 547 940\n",
      " 215 853 805  28 104  67 311 429 941 950 603 971 486 504 497 670 459 559\n",
      " 940   9 829 888 773 784 782 579 714 274 146 245 761 256 326 264 827 690\n",
      " 973 130  91 615 301 361 614 572  92 303 304 799 362 222 371 547 735 121\n",
      " 165 509  79 556 821 927 969 551 375 100 524 357 584 737 716 239  41 139\n",
      " 445 484 739 942 540 462 737 998 407 491 646 607 735 599  69 955  53 514\n",
      "   6 764 918 389 854 559 927 587 344 428  45 188 411  59  28 757 511 274\n",
      " 443 715 659 609 960 775 743 580 196 546 765 279 273 230 587 535 865  62\n",
      "  27 365 226 252 914 295 664 842 211 468 733 738 921 789 973 903 504 741\n",
      " 231 624 635 483 929 133 871 702 351 293 592 521 541 601 436 249 376  19\n",
      "  81 717 733 988 270 531 210 237 918 599 979 259  40   0 422 261 363  28\n",
      " 421 171 235 908 664 192 592 846 444 705 726 747 593 206 140 865 959 168\n",
      " 441 353 357 756 669 197 168 125 234  31 424 440 455 242 619 260 492  42\n",
      " 165 299 608 887 840 716 245 385 969 360 641 749 695 351 703 139 919 809\n",
      " 167 834 513 337 805 329  87 340 132 544 748 709 517 816 526 524 210 251\n",
      " 854 945 384 241 792  96 463 897 318 876 845 964  55 345 573 408 315 573\n",
      " 191 887 642 343 658 888 352 378 106 822 191 678 129 164   4 982 319 602\n",
      " 552 389 497 341 152 678 350   2  39 104 605 243 469 804  59 387  78  84\n",
      " 722 297 271 295 682  60 678  23 835 997 344 493 957 911 776 216 323 864\n",
      " 125 257 696 760 683 787 252 323 460 267 208 533 554 682 939 298 334 130\n",
      "  46 450 835 469 702 733 744 621 797 672 454 944 773 965 234 425 770  14\n",
      "  17 965 402 516 107 310 257 590  28 446 573 369 270 224  34 661 918 698\n",
      " 855 124 862 346 189 122 691 323 761 357 665 638 754 478  18 800 706 447\n",
      " 226 756  25 588 149 507 603 110 947 158 944 652 250 924 522 969   2 922\n",
      " 999 234 588 775 532 307 305 869 455 483  30 992 910 232 109 273 502  48\n",
      " 807 290  36 527 347 963 779 472 638 149 575  30 292 973 648 324 640 976\n",
      " 380 285  57 197 807 264 414 938 397 462 937 750 780 172 319 861 994 138\n",
      " 530 176 451 891 741 495 656 230  98 173 869 325 520 488 278 650 381 281\n",
      " 648 400 760 230 544 298 523 882 153 540 744 805 457 163 813 186 758 726\n",
      " 353 906 997 257 156 966 533 763 555 379 578 378 873   6 304 694 635 555\n",
      " 544 805 712 522 888 849 584 209 409 276 101  44 936 521 382 360 567 237\n",
      " 341 587 890 304 911 152 799 569  19 173 138 408 516 995 423 225 452  71\n",
      " 317  74 891 421 461 909 913 592 731 139 349 486 815 973 975 670 475 355\n",
      " 275 448 224  66 192 295 242  93 261 370 526 947 570 972 938  14 731 429\n",
      " 860 536 176 834 555 674 450 229 237 922 366 777  74 401 211  73 851 811\n",
      " 910 952 493 861 819 197  86 760 495 956  11 368  62 658 913 100 682 928\n",
      " 189 758 187 540 301 535 808 450 807 114 674 757 206 831 516 812 793 501\n",
      " 411 691 509 642 802   9 358 442 593  35 665 988 594  13 563 845 103 766\n",
      " 293 888 233 590 843   6 592 248 581 620 579 146 208 566 628 450 503 779\n",
      " 937 200 420 699 651 220 246 174 278 113 445 601 655 970 506 355 705 440\n",
      " 260 822 971 686  81 808 506 902  11 658 556 118 344 324 857 115  67 733\n",
      " 583 417 508 347 546 724 351 312 463 402  49 359 419 797 850 312 988 143\n",
      " 436 523 107 321 461 177 466 437 162 860 270 448  71 356 838 678  27 947\n",
      " 172 447 438 890   2 745 210 181 871 561 280 697 110 985 545  20 837 996\n",
      " 389 788 898 563  29 215 153 533 973 415 519 207  16 674 678 396 399  14\n",
      " 884 642 441 321 479 897 627 508 381 822 140  74 638 337 236 796 245 184\n",
      " 510 924 486 367 794 338  12 359 674 197 704 621 382 916 312  97 990 605\n",
      " 752 867 952 734 737 932 667 201 685 133 657 334 936 525  96 232 912 623\n",
      " 558  84 563 806 284 721 711 481 550 891 179 508 194 803 865 707 673 294\n",
      " 157 206 102 838 269 898 322 926 987 939 985 884 216 371 897 360 687 312\n",
      " 899 829 154 928 623 449 290 662 281 879 620 724 814 833 491 463 522 837\n",
      " 578 325 331 908 235 316  19 153 942 799 877 540 733   3 376 641 827 359\n",
      " 536 417  91 599 645 781 876 245   0 192  73 595 848 228  90  54 916  11\n",
      " 451 930 476 699 781 417 452 659 661  97]\n",
      "Correct predictions: 2 out of 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# axis=1 means we're finding the maximum value along the second dimension (columns)\n",
    "# For a 2D tensor with shape (batch_size, num_classes), axis=1 gives us the class index\n",
    "# with the highest probability for each sample in the batch\n",
    "predictions = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Labels:\", labels.cpu().numpy())\n",
    "print(\"Correct predictions:\", (predictions == labels.cpu().numpy()).sum(), \"out of\", len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9683, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss = loss_fct(outputs, labels)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
