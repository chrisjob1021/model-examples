{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c606b45ab794aa0bc4a565eaf315d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "train_dataset = load_dataset(\"imagenet-1k\", split=\"train\").shuffle(seed=42)\n",
    "eval_dataset = load_dataset(\"imagenet-1k\", split=\"validation\").shuffle(seed=42)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the data augmentation and preprocessing pipeline for training images\n",
    "train_transform = T.Compose([\n",
    "    T.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),  # Ensure 3 channels (convert grayscale to RGB)\n",
    "    T.RandomResizedCrop(224, scale=(0.08, 1.0)),      # Randomly crop and resize to 224x224 (simulates zoom/scale)\n",
    "    T.RandomHorizontalFlip(),                         # Randomly flip images horizontally (augmentation)\n",
    "    T.RandAugment(num_ops=2, magnitude=9),            # Apply 2 random augmentations with magnitude 9 (extra augmentation)\n",
    "    T.ToTensor(),                                     # Convert PIL Image or numpy.ndarray to tensor and scale to [0, 1]\n",
    "    T.Normalize(mean, std),                           # Normalize using ImageNet mean and std\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1)),       # Randomly erase a rectangle region (extra augmentation, 25% chance)\n",
    "])\n",
    "\n",
    "# Define the preprocessing pipeline for evaluation images (no heavy augmentation)\n",
    "eval_transform = T.Compose([\n",
    "    T.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),  # Ensure 3 channels (convert grayscale to RGB)\n",
    "    T.Resize(256),                                    # Resize shorter side to 256 pixels\n",
    "    T.CenterCrop(224),                                # Crop the center 224x224 region\n",
    "    T.ToTensor(),                                     # Convert to tensor and scale to [0, 1]\n",
    "    T.Normalize(mean, std),                           # Normalize using ImageNet mean and std\n",
    "])\n",
    "\n",
    "def train_transform_fn(examples):\n",
    "    # Handle both single examples and batches\n",
    "    if isinstance(examples['image'], list):\n",
    "        # Batch processing\n",
    "        examples[\"pixel_values\"] = [train_transform(image) for image in examples[\"image\"]]\n",
    "    else:\n",
    "        # Single example processing  \n",
    "        examples[\"pixel_values\"] = train_transform(examples[\"image\"])\n",
    "    \n",
    "    # Remove the original image to avoid DataLoader issues\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "def eval_transform_fn(examples):\n",
    "    # Handle both single examples and batches\n",
    "    if isinstance(examples['image'], list):\n",
    "        # Batch processing\n",
    "        examples[\"pixel_values\"] = [eval_transform(image) for image in examples[\"image\"]]\n",
    "    else:\n",
    "        # Single example processing\n",
    "        examples[\"pixel_values\"] = eval_transform(examples[\"image\"])\n",
    "    \n",
    "    # Remove the original image to avoid DataLoader issues\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# Now you can use these with with_transform()\n",
    "train_dataset = train_dataset.with_transform(train_transform_fn)\n",
    "eval_dataset = eval_dataset.with_transform(eval_transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'label': 126,\n",
       "  'pixel_values': tensor([[[ 0.3652,  0.4166,  0.4851,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           [ 0.4166,  0.4679,  0.5707,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           [ 0.5707,  0.6221,  0.7077,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ...,  1.4269,  1.4440,  1.4269],\n",
       "           [-2.1179, -2.1179, -2.1179,  ...,  1.2728,  1.3413,  1.4098],\n",
       "           [-2.1179, -2.1179, -2.1179,  ...,  1.2043,  1.3070,  1.3927]],\n",
       "  \n",
       "          [[ 0.2402,  0.2752,  0.3277,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           [ 0.2752,  0.3102,  0.3803,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           [ 0.3627,  0.3978,  0.4678,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ...,  1.5882,  1.6057,  1.5882],\n",
       "           [-2.0357, -2.0357, -2.0357,  ...,  1.4307,  1.5007,  1.5707],\n",
       "           [-2.0357, -2.0357, -2.0357,  ...,  1.3606,  1.4657,  1.5532]],\n",
       "  \n",
       "          [[ 0.1128,  0.1825,  0.3045,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           [ 0.1476,  0.2173,  0.3219,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           [ 0.1999,  0.2871,  0.4091,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ...,  1.7685,  1.8208,  1.8208],\n",
       "           [-1.8044, -1.8044, -1.8044,  ...,  1.6465,  1.7337,  1.8034],\n",
       "           [-1.8044, -1.8044, -1.8044,  ...,  1.5768,  1.6988,  1.8034]]])},\n",
       " {'label': 348,\n",
       "  'pixel_values': tensor([[[ 1.6838,  1.5639,  1.5639,  ...,  0.1254,  0.1254,  0.0912],\n",
       "           [ 1.2728,  1.1700,  1.2043,  ...,  0.0056, -0.0458, -0.0458],\n",
       "           [ 0.8447,  0.7248,  0.6221,  ..., -0.0801, -0.1999, -0.2513],\n",
       "           ...,\n",
       "           [-0.1486, -0.2171, -0.1999,  ...,  1.4440,  1.4098,  0.6221],\n",
       "           [-0.1486, -0.1657, -0.1657,  ...,  1.1872,  1.2728,  0.4508],\n",
       "           [-0.0801, -0.0972, -0.2171,  ...,  0.8447,  0.9474,  0.4851]],\n",
       "  \n",
       "          [[ 1.7983,  1.6758,  1.6758,  ...,  0.1702,  0.1527,  0.1352],\n",
       "           [ 1.3957,  1.2731,  1.2906,  ...,  0.0476,  0.0126,  0.0126],\n",
       "           [ 0.9055,  0.8004,  0.6779,  ..., -0.0749, -0.1625, -0.1975],\n",
       "           ...,\n",
       "           [-0.2850, -0.3200, -0.3200,  ...,  1.0455,  1.0280,  0.1877],\n",
       "           [-0.3025, -0.3200, -0.2850,  ...,  0.7829,  0.9055,  0.0651],\n",
       "           [-0.2675, -0.2850, -0.3725,  ...,  0.4328,  0.5903,  0.1176]],\n",
       "  \n",
       "          [[ 1.4374,  1.2805,  1.2805,  ..., -0.2707, -0.2707, -0.2881],\n",
       "           [ 0.9668,  0.8622,  0.8971,  ..., -0.3927, -0.4275, -0.4275],\n",
       "           [ 0.4614,  0.3568,  0.2696,  ..., -0.5495, -0.6193, -0.6715],\n",
       "           ...,\n",
       "           [-0.8458, -0.8633, -0.8284,  ...,  0.1476,  0.0605, -0.8981],\n",
       "           [-0.8807, -0.8284, -0.7936,  ..., -0.0441, -0.0267, -1.0201],\n",
       "           [-0.7761, -0.7064, -0.8110,  ..., -0.3230, -0.3055, -0.8807]]])})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0], eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([3, 224, 224]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['pixel_values'].shape, eval_dataset[0]['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RGB conversion with actual grayscale image...\n",
      "Original grayscale image mode: L\n",
      "Original grayscale image size: (400, 300)\n",
      "\n",
      "Testing train transform on grayscale image:\n",
      "âœ… Train transform result shape: torch.Size([3, 224, 224])\n",
      "âœ… Successfully converted to 3 channels\n",
      "âœ… All 3 channels identical (as expected): False\n",
      "\n",
      "Testing eval transform on grayscale image:\n",
      "âœ… Eval transform result shape: torch.Size([3, 224, 224])\n",
      "âœ… Successfully converted to 3 channels\n",
      "âœ… All 3 channels identical (as expected): False\n",
      "\n",
      "ðŸŽ¯ Grayscale to RGB conversion test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test with actual grayscale image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Testing RGB conversion with actual grayscale image...\")\n",
    "\n",
    "# Create a test grayscale image\n",
    "gray_array = np.random.randint(0, 256, (300, 400), dtype=np.uint8)\n",
    "grayscale_image = Image.fromarray(gray_array, mode='L')  # 'L' mode = grayscale\n",
    "\n",
    "print(f\"Original grayscale image mode: {grayscale_image.mode}\")\n",
    "print(f\"Original grayscale image size: {grayscale_image.size}\")\n",
    "\n",
    "# Test train transform on grayscale image\n",
    "print(\"\\nTesting train transform on grayscale image:\")\n",
    "try:\n",
    "    train_result = train_transform(grayscale_image)\n",
    "    print(f\"âœ… Train transform result shape: {train_result.shape}\")\n",
    "    print(f\"âœ… Successfully converted to {train_result.shape[0]} channels\")\n",
    "    \n",
    "    # Verify all 3 channels have the same values (since it was grayscale)\n",
    "    channel_equality = torch.allclose(train_result[0], train_result[1]) and torch.allclose(train_result[1], train_result[2])\n",
    "    print(f\"âœ… All 3 channels identical (as expected): {channel_equality}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in train transform: {e}\")\n",
    "\n",
    "# Test eval transform on grayscale image  \n",
    "print(\"\\nTesting eval transform on grayscale image:\")\n",
    "try:\n",
    "    eval_result = eval_transform(grayscale_image)\n",
    "    print(f\"âœ… Eval transform result shape: {eval_result.shape}\")\n",
    "    print(f\"âœ… Successfully converted to {eval_result.shape[0]} channels\")\n",
    "    \n",
    "    # Verify all 3 channels have the same values (since it was grayscale)\n",
    "    channel_equality = torch.allclose(eval_result[0], eval_result[1]) and torch.allclose(eval_result[1], eval_result[2])\n",
    "    print(f\"âœ… All 3 channels identical (as expected): {channel_equality}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in eval transform: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Grayscale to RGB conversion test completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
