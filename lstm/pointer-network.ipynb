{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, input_dim = 1):\n",
    "        super().__init__()\n",
    "        # input_dim is the size of each element’s feature vector—what the encoder LSTM sees at every time-step.\n",
    "        # In the little “sort a list of real numbers” demo we built, each token is just a single scalar (e.g., 0.42).\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
    "        # With batch_first=True: The input and output tensors are expected to be of shape (batch, seq_len, input_size). \n",
    "        # With bidirectional=True: The LSTM processes the input sequence in both forward and backward directions. \n",
    "        #   This means for each time step, the output contains information from both past and future contexts.\n",
    "        # The output of the bidirectional LSTM is twice the hidden dimension.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1) # add feature dimension -> (B, T, 1)\n",
    "        print(x)\n",
    "        # x is the input to the LSTM, which is a tensor of shape (batch_size, seq_len, input_size).\n",
    "        # LSTM expects a 3D input tensor of shape (batch_size, seq_len, input_size).\n",
    "        h, _ = self.lstm(x)\n",
    "        # h is the output of the LSTM, which is a tensor of shape (batch_size, seq_len, hidden_dim * 2).\n",
    "        # The final layer of the encoder is a linear layer that maps the output of the LSTM to a vector of size hidden_dim.\n",
    "        return h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=hidden_dim * 2, hidden_size=hidden_dim)\n",
    "        self.W1 = nn.Linear(2 * hidden_dim, hidden_dim, bias=False) # input is bidirectional\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, enc_out, targets=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        enc_out : (B, T, 2H)\n",
    "        targets  : (B, T) or None  (teacher forcing indices)\n",
    "\n",
    "        B = batch size\n",
    "        T = sequence length\n",
    "        H = hidden dimension\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : (B, T, T)  --- unnormalised pointer scores\n",
    "        \"\"\"\n",
    "\n",
    "        B, T, _ = enc_out.size()\n",
    "        h_t = torch.zeros(B, self.hidden_dim, device=enc_out.device)\n",
    "        c_t = torch.zeros_like(h_t)\n",
    "\n",
    "        logits = []\n",
    "        mask = torch.zeros(B, T, device=enc_out.device)\n",
    "\n",
    "        enc_proj = self.W1(enc_out) # (B, T, H)\n",
    "        enc_out_idx = torch.arange(B, device=enc_out.device) # (B) This creates a tensor [0, 1, ..., B-1] to index each batch.\n",
    "\n",
    "        for i in range(T):\n",
    "            if i == 0:\n",
    "                # first time step, use the mean of the encoder outputs\n",
    "                ctx = enc_out.mean(dim=1) # (B, 2H)\n",
    "            else:\n",
    "                # subsequent time steps, use the previously chosen embedding as context\n",
    "                idx = targets[:, i-1] if targets is not None else prev_idx\n",
    "                ctx = enc_out[enc_out_idx, idx] # (B, H) For each item in the batch, select the encoder output at the position given by idx.\n",
    "                                                #        This gives you a context vector for each batch item, based on the previously selected position.\n",
    "            \n",
    "            # LSTMCell\n",
    "            # Inputs: input, (h_0, c_0)\n",
    "            # input of shape (batch, input_size) or (input_size): tensor containing input features\n",
    "            # h_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial hidden state\n",
    "            # c_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial cell state\n",
    "\n",
    "            # Outputs: (h_1, c_1)\n",
    "            # h_1 of shape (batch, hidden_size) or (hidden_size): tensor containing the next hidden state\n",
    "            # c_1 of shape (batch, hidden_size) or (hidden_size): tensor containing the next cell state\n",
    "\n",
    "            # current decoder hidden- and cell-state BEFORE we look at position i\n",
    "            h_t, c_t = self.lstm_cell(ctx, (h_t, c_t))   # (B, H)\n",
    "            W2_proj = self.W2(h_t).unsqueeze(1) # (B, 1, H)\n",
    "            tanh = torch.tanh(enc_proj + W2_proj) # (B, T, H)\n",
    "            u_i = self.v(tanh).squeeze(-1) # (B, T)\n",
    "            u_i = u_i - 1e9 * mask\n",
    "            print (\"u_i\", u_i)\n",
    "            logits.append(u_i)\n",
    "\n",
    "            prev_idx = torch.argmax(u_i, dim=-1)\n",
    "            print (\"prev_idx\", prev_idx)\n",
    "            mask[enc_out_idx, prev_idx] = 1 # mask the selected index for future predictions\n",
    "\n",
    "        # In this context, torch.stack(logits, dim=1) is used to combine a list of tensors (each representing the logits at a different decoding step) into a single tensor with a new dimension.\n",
    "        # If you have T decoding steps, and each u_i is shape (B, T), then after stacking, you get a tensor of shape (B, T, T).\n",
    "        logits = torch.stack(logits, dim=-1) # (B, T, T)\n",
    "        return logits\n",
    "\n",
    "class PointerNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        enc_out = self.encoder(x) # (B, T, 2H)\n",
    "        print (\"inputs\", x)\n",
    "        print (\"targets\", targets)\n",
    "        logits = self.decoder(enc_out, targets) # (B, T, T)\n",
    "        return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Synthetic dataset helpers\n",
    "# --------------------------\n",
    "def gen_batch(batch_sz, seq_len = 5):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    inputs : (B, T)  float32  -- unsorted numbers\n",
    "    targets: (B, T)  long     -- permutation (indices) that would sort each row ascending\n",
    "    \"\"\"\n",
    "    inputs = torch.rand(batch_sz, seq_len)\n",
    "    targets = torch.argsort(inputs, dim=1)  # ascending order indices\n",
    "    return inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE cpu\n",
      "tensor([[[0.9461],\n",
      "         [0.4819],\n",
      "         [0.4461],\n",
      "         [0.3306],\n",
      "         [0.0337]]])\n",
      "inputs tensor([[0.9461, 0.4819, 0.4461, 0.3306, 0.0337]])\n",
      "targets tensor([[4, 3, 2, 1, 0]])\n",
      "u_i tensor([[0.0481, 0.0406, 0.0338, 0.0228, 0.0076]], grad_fn=<SubBackward0>)\n",
      "prev_idx tensor([0])\n",
      "u_i tensor([[-1.0000e+09,  4.0336e-02,  3.3614e-02,  2.2538e-02,  7.3943e-03]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "prev_idx tensor([1])\n",
      "u_i tensor([[-1.0000e+09, -1.0000e+09,  3.3183e-02,  2.2087e-02,  6.9283e-03]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "prev_idx tensor([2])\n",
      "u_i tensor([[-1.0000e+09, -1.0000e+09, -1.0000e+09,  2.1626e-02,  6.4527e-03]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "prev_idx tensor([3])\n",
      "u_i tensor([[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,  6.0431e-03]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "prev_idx tensor([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if True:\n",
    "    # --------------------------\n",
    "    # Model\n",
    "    # --------------------------\n",
    "    model = PointerNetwork(hidden_dim=1).to(DEVICE)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    batch_sz = 1\n",
    "    seq_len = 5\n",
    "    model.train() \n",
    "    inputs, targets = gen_batch(batch_sz, seq_len)\n",
    "    logits = model(inputs, targets)\n",
    "\n",
    "# inputs, targets, logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
