{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pointer Networks (Vinyals et al., 2015)\n",
    "# Implementation based on the paper: https://arxiv.org/pdf/1506.03134\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, input_dim = 1):\n",
    "        super().__init__()\n",
    "        # input_dim is the size of each element’s feature vector—what the encoder LSTM sees at every time-step.\n",
    "        # In the little “sort a list of real numbers” demo we built, each token is just a single scalar (e.g., 0.42).\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
    "        # With batch_first=True: The input and output tensors are expected to be of shape (batch, seq_len, input_size). \n",
    "        # With bidirectional=True: The LSTM processes the input sequence in both forward and backward directions. \n",
    "        #   This means for each time step, the output contains information from both past and future contexts.\n",
    "        # The output of the bidirectional LSTM is twice the hidden dimension.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1) # add feature dimension -> (B, T, 1)\n",
    "        # print(x)\n",
    "        # x is the input to the LSTM, which is a tensor of shape (batch_size, seq_len, input_size).\n",
    "        # LSTM expects a 3D input tensor of shape (batch_size, seq_len, input_size).\n",
    "        h, _ = self.lstm(x)\n",
    "        # h is the output of the LSTM, which is a tensor of shape (batch_size, seq_len, hidden_dim * 2).\n",
    "        # The final layer of the encoder is a linear layer that maps the output of the LSTM to a vector of size hidden_dim.\n",
    "        return h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=hidden_dim * 2, hidden_size=hidden_dim)\n",
    "        self.W1 = nn.Linear(2 * hidden_dim, hidden_dim, bias=False) # input is bidirectional\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, enc_out, targets=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        enc_out : (B, T, 2H)\n",
    "        targets  : (B, T) or None  (teacher forcing indices)\n",
    "\n",
    "        B = batch size\n",
    "        T = sequence length\n",
    "        H = hidden dimension\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : (B, T, T)  --- unnormalised pointer scores\n",
    "        \"\"\"\n",
    "\n",
    "        B, T, _ = enc_out.size()\n",
    "        h_t = torch.zeros(B, self.hidden_dim, device=enc_out.device)\n",
    "        c_t = torch.zeros_like(h_t)\n",
    "\n",
    "        logits = []\n",
    "        mask = torch.zeros(B, T, device=enc_out.device)\n",
    "\n",
    "        enc_proj = self.W1(enc_out) # (B, T, H)\n",
    "        enc_out_idx = torch.arange(B, device=enc_out.device) # (B) This creates a tensor [0, 1, ..., B-1] to index each batch.\n",
    "\n",
    "        for i in range(T):\n",
    "            if i == 0:\n",
    "                # first time step, use the mean of the encoder outputs\n",
    "                ctx = enc_out.mean(dim=1) # (B, 2H)\n",
    "            else:\n",
    "                # subsequent time steps, use the previously chosen embedding as context\n",
    "                idx = targets[:, i-1] if targets is not None else prev_idx\n",
    "                ctx = enc_out[enc_out_idx, idx] # (B, H) For each item in the batch, select the encoder output at the position given by idx.\n",
    "                                                #        This gives you a context vector for each batch item, based on the previously selected position.\n",
    "                mask[enc_out_idx, idx] = 1 # mask the selected index for future predictions\n",
    "            \n",
    "            # LSTMCell\n",
    "            # Inputs: input, (h_0, c_0)\n",
    "            # input of shape (batch, input_size) or (input_size): tensor containing input features\n",
    "            # h_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial hidden state\n",
    "            # c_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial cell state\n",
    "\n",
    "            # Outputs: (h_1, c_1)\n",
    "            # h_1 of shape (batch, hidden_size) or (hidden_size): tensor containing the next hidden state\n",
    "            # c_1 of shape (batch, hidden_size) or (hidden_size): tensor containing the next cell state\n",
    "\n",
    "            # current decoder hidden- and cell-state BEFORE we look at position i\n",
    "            h_t, c_t = self.lstm_cell(ctx, (h_t, c_t))   # (B, H)\n",
    "            W2_proj = self.W2(h_t).unsqueeze(1) # (B, 1, H)\n",
    "            tanh = torch.tanh(enc_proj + W2_proj) # (B, T, H)\n",
    "            u_i = self.v(tanh).squeeze(-1) # (B, T)\n",
    "            u_i = u_i - 1e9 * mask\n",
    "            # print (\"u_i\", u_i)\n",
    "            logits.append(u_i)\n",
    "\n",
    "            prev_idx = torch.argmax(u_i, dim=-1)\n",
    "            # print (\"prev_idx\", prev_idx)\n",
    "            # mask[enc_out_idx, prev_idx] = 1 # mask the selected index for future predictions\n",
    "\n",
    "        # In this context, torch.stack(logits, dim=1) is used to combine a list of tensors (each representing the logits at a different decoding step) into a single tensor with a new dimension.\n",
    "        # If you have T decoding steps, and each u_i is shape (B, T), then after stacking, you get a tensor of shape (B, T, T).\n",
    "        logits = torch.stack(logits, dim=1) # (B, T, T)\n",
    "        return logits\n",
    "\n",
    "class PointerNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        enc_out = self.encoder(x) # (B, T, 2H)\n",
    "        # print (\"inputs\", x)\n",
    "        # print (\"targets\", targets)\n",
    "        logits = self.decoder(enc_out, targets) # (B, T, T)\n",
    "        return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Synthetic dataset helpers\n",
    "# --------------------------\n",
    "def gen_batch(batch_sz, seq_len = 5):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    inputs : (B, T)  float32  -- unsorted numbers\n",
    "    targets: (B, T)  long     -- permutation (indices) that would sort each row ascending\n",
    "    \"\"\"\n",
    "    inputs = torch.rand(batch_sz, seq_len)\n",
    "    targets = torch.argsort(inputs, dim=1)  # ascending order indices\n",
    "    return inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1 | loss 1.513 | val acc   8.6%\n",
      "step    2 | loss 1.511 | val acc  14.3%\n",
      "step    3 | loss 1.509 | val acc  17.7%\n",
      "step    4 | loss 1.507 | val acc  22.1%\n",
      "step    5 | loss 1.506 | val acc  25.2%\n",
      "step    6 | loss 1.504 | val acc  28.6%\n",
      "step    7 | loss 1.502 | val acc  30.5%\n",
      "step    8 | loss 1.500 | val acc  35.8%\n",
      "step    9 | loss 1.498 | val acc  41.4%\n",
      "step   10 | loss 1.496 | val acc  42.7%\n",
      "step   11 | loss 1.493 | val acc  44.9%\n",
      "step   12 | loss 1.491 | val acc  48.2%\n",
      "step   13 | loss 1.489 | val acc  45.6%\n",
      "step   14 | loss 1.485 | val acc  47.9%\n",
      "step   15 | loss 1.484 | val acc  49.3%\n",
      "step   16 | loss 1.479 | val acc  51.2%\n",
      "step   17 | loss 1.477 | val acc  52.9%\n",
      "step   18 | loss 1.473 | val acc  52.7%\n",
      "step   19 | loss 1.469 | val acc  53.6%\n",
      "step   20 | loss 1.465 | val acc  56.4%\n",
      "step   21 | loss 1.461 | val acc  56.8%\n",
      "step   22 | loss 1.457 | val acc  57.6%\n",
      "step   23 | loss 1.451 | val acc  61.3%\n",
      "step   24 | loss 1.446 | val acc  62.1%\n",
      "step   25 | loss 1.440 | val acc  61.3%\n",
      "step   26 | loss 1.435 | val acc  62.3%\n",
      "step   27 | loss 1.427 | val acc  64.1%\n",
      "step   28 | loss 1.423 | val acc  64.6%\n",
      "step   29 | loss 1.413 | val acc  65.6%\n",
      "step   30 | loss 1.405 | val acc  64.1%\n",
      "step   31 | loss 1.398 | val acc  64.3%\n",
      "step   32 | loss 1.389 | val acc  67.8%\n",
      "step   33 | loss 1.378 | val acc  71.7%\n",
      "step   34 | loss 1.368 | val acc  74.5%\n",
      "step   35 | loss 1.358 | val acc  75.0%\n",
      "step   36 | loss 1.347 | val acc  76.8%\n",
      "step   37 | loss 1.330 | val acc  77.3%\n",
      "step   38 | loss 1.318 | val acc  77.3%\n",
      "step   39 | loss 1.306 | val acc  80.7%\n",
      "step   40 | loss 1.291 | val acc  77.0%\n",
      "step   41 | loss 1.273 | val acc  77.9%\n",
      "step   42 | loss 1.261 | val acc  76.8%\n",
      "step   43 | loss 1.243 | val acc  77.5%\n",
      "step   44 | loss 1.221 | val acc  76.2%\n",
      "step   45 | loss 1.204 | val acc  72.9%\n",
      "step   46 | loss 1.188 | val acc  68.8%\n",
      "step   47 | loss 1.164 | val acc  69.0%\n",
      "step   48 | loss 1.147 | val acc  64.0%\n",
      "step   49 | loss 1.130 | val acc  65.5%\n",
      "step   50 | loss 1.110 | val acc  64.6%\n",
      "step   51 | loss 1.078 | val acc  63.9%\n",
      "step   52 | loss 1.065 | val acc  64.4%\n",
      "step   53 | loss 1.035 | val acc  61.8%\n",
      "step   54 | loss 1.020 | val acc  56.7%\n",
      "step   55 | loss 1.004 | val acc  61.2%\n",
      "step   56 | loss 0.980 | val acc  61.8%\n",
      "step   57 | loss 0.950 | val acc  60.1%\n",
      "step   58 | loss 0.940 | val acc  63.9%\n",
      "step   59 | loss 0.912 | val acc  61.5%\n",
      "step   60 | loss 0.892 | val acc  63.7%\n",
      "step   61 | loss 0.863 | val acc  63.3%\n",
      "step   62 | loss 0.849 | val acc  65.6%\n",
      "step   63 | loss 0.824 | val acc  63.4%\n",
      "step   64 | loss 0.814 | val acc  65.2%\n",
      "step   65 | loss 0.793 | val acc  64.2%\n",
      "step   66 | loss 0.774 | val acc  65.3%\n",
      "step   67 | loss 0.748 | val acc  66.0%\n",
      "step   68 | loss 0.725 | val acc  67.9%\n",
      "step   69 | loss 0.717 | val acc  66.7%\n",
      "step   70 | loss 0.705 | val acc  66.3%\n",
      "step   71 | loss 0.684 | val acc  67.6%\n",
      "step   72 | loss 0.671 | val acc  67.4%\n",
      "step   73 | loss 0.659 | val acc  70.3%\n",
      "step   74 | loss 0.633 | val acc  66.6%\n",
      "step   75 | loss 0.623 | val acc  72.1%\n",
      "step   76 | loss 0.609 | val acc  69.5%\n",
      "step   77 | loss 0.604 | val acc  69.2%\n",
      "step   78 | loss 0.588 | val acc  70.1%\n",
      "step   79 | loss 0.570 | val acc  67.3%\n",
      "step   80 | loss 0.567 | val acc  73.2%\n",
      "step   81 | loss 0.553 | val acc  72.4%\n",
      "step   82 | loss 0.549 | val acc  73.2%\n",
      "step   83 | loss 0.532 | val acc  74.0%\n",
      "step   84 | loss 0.528 | val acc  73.4%\n",
      "step   85 | loss 0.513 | val acc  73.2%\n",
      "step   86 | loss 0.527 | val acc  71.9%\n",
      "step   87 | loss 0.512 | val acc  73.0%\n",
      "step   88 | loss 0.500 | val acc  72.6%\n",
      "step   89 | loss 0.487 | val acc  73.9%\n",
      "step   90 | loss 0.485 | val acc  74.1%\n",
      "step   91 | loss 0.477 | val acc  72.5%\n",
      "step   92 | loss 0.476 | val acc  74.8%\n",
      "step   93 | loss 0.461 | val acc  73.0%\n",
      "step   94 | loss 0.462 | val acc  73.8%\n",
      "step   95 | loss 0.449 | val acc  73.2%\n",
      "step   96 | loss 0.445 | val acc  73.5%\n",
      "step   97 | loss 0.440 | val acc  74.6%\n",
      "step   98 | loss 0.449 | val acc  73.8%\n",
      "step   99 | loss 0.446 | val acc  76.0%\n",
      "step  100 | loss 0.437 | val acc  73.9%\n",
      "step  101 | loss 0.423 | val acc  73.7%\n",
      "step  102 | loss 0.418 | val acc  73.7%\n",
      "step  103 | loss 0.414 | val acc  76.2%\n",
      "step  104 | loss 0.423 | val acc  73.7%\n",
      "step  105 | loss 0.419 | val acc  74.5%\n",
      "step  106 | loss 0.402 | val acc  74.0%\n",
      "step  107 | loss 0.403 | val acc  73.0%\n",
      "step  108 | loss 0.394 | val acc  77.0%\n",
      "step  109 | loss 0.406 | val acc  76.7%\n",
      "step  110 | loss 0.406 | val acc  77.5%\n",
      "step  111 | loss 0.388 | val acc  75.6%\n",
      "step  112 | loss 0.393 | val acc  77.4%\n",
      "step  113 | loss 0.374 | val acc  76.4%\n",
      "step  114 | loss 0.374 | val acc  78.0%\n",
      "step  115 | loss 0.379 | val acc  76.6%\n",
      "step  116 | loss 0.374 | val acc  76.6%\n",
      "step  117 | loss 0.373 | val acc  76.3%\n",
      "step  118 | loss 0.357 | val acc  75.4%\n",
      "step  119 | loss 0.370 | val acc  77.9%\n",
      "step  120 | loss 0.369 | val acc  78.4%\n",
      "step  121 | loss 0.355 | val acc  76.3%\n",
      "step  122 | loss 0.361 | val acc  79.6%\n",
      "step  123 | loss 0.360 | val acc  77.8%\n",
      "step  124 | loss 0.348 | val acc  79.0%\n",
      "step  125 | loss 0.348 | val acc  78.4%\n",
      "step  126 | loss 0.333 | val acc  79.8%\n",
      "step  127 | loss 0.342 | val acc  80.9%\n",
      "step  128 | loss 0.343 | val acc  78.1%\n",
      "step  129 | loss 0.347 | val acc  79.6%\n",
      "step  130 | loss 0.330 | val acc  81.6%\n",
      "step  131 | loss 0.342 | val acc  77.7%\n",
      "step  132 | loss 0.339 | val acc  80.8%\n",
      "step  133 | loss 0.336 | val acc  80.3%\n",
      "step  134 | loss 0.325 | val acc  78.6%\n",
      "step  135 | loss 0.337 | val acc  82.5%\n",
      "step  136 | loss 0.323 | val acc  79.2%\n",
      "step  137 | loss 0.335 | val acc  77.5%\n",
      "step  138 | loss 0.333 | val acc  80.2%\n",
      "step  139 | loss 0.321 | val acc  81.4%\n",
      "step  140 | loss 0.308 | val acc  82.5%\n",
      "step  141 | loss 0.316 | val acc  81.2%\n",
      "step  142 | loss 0.306 | val acc  81.5%\n",
      "step  143 | loss 0.314 | val acc  82.3%\n",
      "step  144 | loss 0.306 | val acc  82.3%\n",
      "step  145 | loss 0.305 | val acc  83.0%\n",
      "step  146 | loss 0.303 | val acc  83.8%\n",
      "step  147 | loss 0.286 | val acc  83.2%\n",
      "step  148 | loss 0.308 | val acc  84.2%\n",
      "step  149 | loss 0.291 | val acc  84.6%\n",
      "step  150 | loss 0.296 | val acc  82.3%\n",
      "step  151 | loss 0.307 | val acc  82.0%\n",
      "step  152 | loss 0.305 | val acc  84.0%\n",
      "step  153 | loss 0.300 | val acc  82.1%\n",
      "step  154 | loss 0.290 | val acc  83.7%\n",
      "step  155 | loss 0.283 | val acc  81.5%\n",
      "step  156 | loss 0.293 | val acc  84.3%\n",
      "step  157 | loss 0.280 | val acc  83.6%\n",
      "step  158 | loss 0.278 | val acc  82.1%\n",
      "step  159 | loss 0.286 | val acc  85.0%\n",
      "step  160 | loss 0.270 | val acc  81.2%\n",
      "step  161 | loss 0.275 | val acc  85.0%\n",
      "step  162 | loss 0.266 | val acc  82.3%\n",
      "step  163 | loss 0.290 | val acc  85.5%\n",
      "step  164 | loss 0.278 | val acc  83.1%\n",
      "step  165 | loss 0.278 | val acc  83.4%\n",
      "step  166 | loss 0.281 | val acc  85.6%\n",
      "step  167 | loss 0.267 | val acc  86.2%\n",
      "step  168 | loss 0.259 | val acc  85.3%\n",
      "step  169 | loss 0.267 | val acc  85.7%\n",
      "step  170 | loss 0.258 | val acc  85.7%\n",
      "step  171 | loss 0.256 | val acc  87.5%\n",
      "step  172 | loss 0.260 | val acc  83.7%\n",
      "step  173 | loss 0.267 | val acc  85.1%\n",
      "step  174 | loss 0.269 | val acc  81.3%\n",
      "step  175 | loss 0.268 | val acc  88.9%\n",
      "step  176 | loss 0.251 | val acc  85.7%\n",
      "step  177 | loss 0.254 | val acc  85.6%\n",
      "step  178 | loss 0.249 | val acc  88.3%\n",
      "step  179 | loss 0.254 | val acc  88.1%\n",
      "step  180 | loss 0.256 | val acc  87.8%\n",
      "step  181 | loss 0.247 | val acc  88.2%\n",
      "step  182 | loss 0.246 | val acc  88.6%\n",
      "step  183 | loss 0.239 | val acc  88.1%\n",
      "step  184 | loss 0.231 | val acc  88.7%\n",
      "step  185 | loss 0.240 | val acc  88.8%\n",
      "step  186 | loss 0.250 | val acc  88.5%\n",
      "step  187 | loss 0.226 | val acc  86.2%\n",
      "step  188 | loss 0.246 | val acc  88.4%\n",
      "step  189 | loss 0.250 | val acc  87.9%\n",
      "step  190 | loss 0.233 | val acc  88.5%\n",
      "step  191 | loss 0.227 | val acc  88.1%\n",
      "step  192 | loss 0.228 | val acc  88.7%\n",
      "step  193 | loss 0.230 | val acc  88.0%\n",
      "step  194 | loss 0.238 | val acc  89.3%\n",
      "step  195 | loss 0.231 | val acc  89.8%\n",
      "step  196 | loss 0.232 | val acc  89.0%\n",
      "step  197 | loss 0.226 | val acc  90.4%\n",
      "step  198 | loss 0.230 | val acc  90.3%\n",
      "step  199 | loss 0.215 | val acc  90.5%\n",
      "step  200 | loss 0.227 | val acc  89.8%\n",
      "step  201 | loss 0.222 | val acc  89.3%\n",
      "step  202 | loss 0.215 | val acc  90.0%\n",
      "step  203 | loss 0.224 | val acc  88.7%\n",
      "step  204 | loss 0.215 | val acc  89.9%\n",
      "step  205 | loss 0.223 | val acc  88.1%\n",
      "step  206 | loss 0.210 | val acc  91.3%\n",
      "step  207 | loss 0.211 | val acc  87.8%\n",
      "step  208 | loss 0.219 | val acc  90.4%\n",
      "step  209 | loss 0.209 | val acc  91.7%\n",
      "step  210 | loss 0.216 | val acc  89.1%\n",
      "step  211 | loss 0.207 | val acc  90.4%\n",
      "step  212 | loss 0.201 | val acc  90.2%\n",
      "step  213 | loss 0.209 | val acc  87.7%\n",
      "step  214 | loss 0.210 | val acc  89.8%\n",
      "step  215 | loss 0.220 | val acc  87.1%\n",
      "step  216 | loss 0.211 | val acc  90.4%\n",
      "step  217 | loss 0.201 | val acc  89.9%\n",
      "step  218 | loss 0.198 | val acc  87.5%\n",
      "step  219 | loss 0.194 | val acc  89.7%\n",
      "step  220 | loss 0.208 | val acc  90.2%\n",
      "step  221 | loss 0.205 | val acc  90.7%\n",
      "step  222 | loss 0.192 | val acc  89.8%\n",
      "step  223 | loss 0.203 | val acc  89.5%\n",
      "step  224 | loss 0.191 | val acc  89.6%\n",
      "step  225 | loss 0.194 | val acc  88.2%\n",
      "step  226 | loss 0.194 | val acc  90.6%\n",
      "step  227 | loss 0.195 | val acc  90.4%\n",
      "step  228 | loss 0.196 | val acc  89.8%\n",
      "step  229 | loss 0.203 | val acc  91.4%\n",
      "step  230 | loss 0.194 | val acc  88.1%\n",
      "step  231 | loss 0.207 | val acc  90.0%\n",
      "step  232 | loss 0.190 | val acc  91.1%\n",
      "step  233 | loss 0.197 | val acc  90.7%\n",
      "step  234 | loss 0.195 | val acc  91.6%\n",
      "step  235 | loss 0.192 | val acc  90.0%\n",
      "step  236 | loss 0.193 | val acc  88.1%\n",
      "step  237 | loss 0.202 | val acc  92.1%\n",
      "step  238 | loss 0.182 | val acc  88.8%\n",
      "step  239 | loss 0.201 | val acc  90.1%\n",
      "step  240 | loss 0.186 | val acc  89.5%\n",
      "step  241 | loss 0.187 | val acc  87.7%\n",
      "step  242 | loss 0.203 | val acc  89.5%\n",
      "step  243 | loss 0.197 | val acc  91.4%\n",
      "step  244 | loss 0.185 | val acc  89.3%\n",
      "step  245 | loss 0.198 | val acc  90.4%\n",
      "step  246 | loss 0.182 | val acc  90.8%\n",
      "step  247 | loss 0.174 | val acc  88.7%\n",
      "step  248 | loss 0.193 | val acc  90.8%\n",
      "step  249 | loss 0.169 | val acc  90.8%\n",
      "step  250 | loss 0.181 | val acc  90.2%\n",
      "step  251 | loss 0.193 | val acc  91.8%\n",
      "step  252 | loss 0.170 | val acc  90.4%\n",
      "step  253 | loss 0.183 | val acc  91.4%\n",
      "step  254 | loss 0.180 | val acc  91.2%\n",
      "step  255 | loss 0.176 | val acc  90.7%\n",
      "step  256 | loss 0.182 | val acc  91.7%\n",
      "step  257 | loss 0.168 | val acc  90.9%\n",
      "step  258 | loss 0.180 | val acc  90.8%\n",
      "step  259 | loss 0.176 | val acc  93.1%\n",
      "step  260 | loss 0.180 | val acc  92.1%\n",
      "step  261 | loss 0.176 | val acc  93.5%\n",
      "step  262 | loss 0.171 | val acc  92.8%\n",
      "step  263 | loss 0.177 | val acc  90.8%\n",
      "step  264 | loss 0.158 | val acc  92.6%\n",
      "step  265 | loss 0.175 | val acc  92.8%\n",
      "step  266 | loss 0.172 | val acc  91.9%\n",
      "step  267 | loss 0.172 | val acc  91.4%\n",
      "step  268 | loss 0.179 | val acc  90.4%\n",
      "step  269 | loss 0.167 | val acc  90.6%\n",
      "step  270 | loss 0.165 | val acc  90.8%\n",
      "step  271 | loss 0.177 | val acc  93.2%\n",
      "step  272 | loss 0.166 | val acc  93.2%\n",
      "step  273 | loss 0.163 | val acc  93.6%\n",
      "step  274 | loss 0.166 | val acc  92.5%\n",
      "step  275 | loss 0.162 | val acc  93.4%\n",
      "step  276 | loss 0.172 | val acc  91.2%\n",
      "step  277 | loss 0.160 | val acc  94.3%\n",
      "step  278 | loss 0.156 | val acc  92.1%\n",
      "step  279 | loss 0.160 | val acc  91.4%\n",
      "step  280 | loss 0.162 | val acc  92.8%\n",
      "step  281 | loss 0.166 | val acc  89.5%\n",
      "step  282 | loss 0.173 | val acc  93.5%\n",
      "step  283 | loss 0.157 | val acc  90.6%\n",
      "step  284 | loss 0.172 | val acc  93.6%\n",
      "step  285 | loss 0.156 | val acc  89.1%\n",
      "step  286 | loss 0.175 | val acc  92.7%\n",
      "step  287 | loss 0.160 | val acc  88.6%\n",
      "step  288 | loss 0.166 | val acc  88.7%\n",
      "step  289 | loss 0.164 | val acc  89.0%\n",
      "step  290 | loss 0.159 | val acc  93.5%\n",
      "step  291 | loss 0.153 | val acc  91.5%\n",
      "step  292 | loss 0.168 | val acc  90.0%\n",
      "step  293 | loss 0.163 | val acc  94.6%\n",
      "step  294 | loss 0.154 | val acc  91.0%\n",
      "step  295 | loss 0.169 | val acc  94.9%\n",
      "step  296 | loss 0.153 | val acc  89.9%\n",
      "step  297 | loss 0.166 | val acc  94.9%\n",
      "step  298 | loss 0.153 | val acc  91.5%\n",
      "step  299 | loss 0.154 | val acc  93.2%\n",
      "step  300 | loss 0.152 | val acc  94.4%\n",
      "step  301 | loss 0.154 | val acc  93.5%\n",
      "step  302 | loss 0.152 | val acc  94.4%\n",
      "step  303 | loss 0.150 | val acc  94.9%\n",
      "step  304 | loss 0.146 | val acc  93.0%\n",
      "step  305 | loss 0.155 | val acc  94.1%\n",
      "step  306 | loss 0.145 | val acc  93.4%\n",
      "step  307 | loss 0.150 | val acc  93.9%\n",
      "step  308 | loss 0.147 | val acc  93.9%\n",
      "step  309 | loss 0.144 | val acc  93.8%\n",
      "step  310 | loss 0.152 | val acc  94.3%\n",
      "step  311 | loss 0.147 | val acc  93.0%\n",
      "step  312 | loss 0.144 | val acc  93.2%\n",
      "step  313 | loss 0.145 | val acc  94.1%\n",
      "step  314 | loss 0.148 | val acc  93.9%\n",
      "step  315 | loss 0.147 | val acc  94.6%\n",
      "step  316 | loss 0.138 | val acc  94.6%\n",
      "step  317 | loss 0.147 | val acc  95.2%\n",
      "step  318 | loss 0.141 | val acc  94.1%\n",
      "step  319 | loss 0.146 | val acc  95.0%\n",
      "step  320 | loss 0.148 | val acc  93.8%\n",
      "step  321 | loss 0.144 | val acc  93.8%\n",
      "step  322 | loss 0.138 | val acc  91.3%\n",
      "step  323 | loss 0.146 | val acc  93.8%\n",
      "step  324 | loss 0.144 | val acc  93.3%\n",
      "step  325 | loss 0.146 | val acc  90.0%\n",
      "step  326 | loss 0.157 | val acc  94.6%\n",
      "step  327 | loss 0.144 | val acc  92.0%\n",
      "step  328 | loss 0.141 | val acc  94.2%\n",
      "step  329 | loss 0.138 | val acc  91.0%\n",
      "step  330 | loss 0.146 | val acc  92.4%\n",
      "step  331 | loss 0.143 | val acc  90.4%\n",
      "step  332 | loss 0.160 | val acc  90.3%\n",
      "step  333 | loss 0.145 | val acc  92.2%\n",
      "step  334 | loss 0.140 | val acc  89.4%\n",
      "step  335 | loss 0.151 | val acc  94.5%\n",
      "step  336 | loss 0.144 | val acc  88.8%\n",
      "step  337 | loss 0.159 | val acc  95.8%\n",
      "step  338 | loss 0.140 | val acc  89.9%\n",
      "step  339 | loss 0.151 | val acc  93.7%\n",
      "step  340 | loss 0.136 | val acc  89.1%\n",
      "step  341 | loss 0.152 | val acc  95.5%\n",
      "step  342 | loss 0.132 | val acc  90.9%\n",
      "step  343 | loss 0.160 | val acc  92.1%\n",
      "step  344 | loss 0.145 | val acc  91.7%\n",
      "step  345 | loss 0.145 | val acc  91.5%\n",
      "step  346 | loss 0.145 | val acc  93.6%\n",
      "step  347 | loss 0.132 | val acc  92.1%\n",
      "step  348 | loss 0.140 | val acc  93.0%\n",
      "step  349 | loss 0.131 | val acc  92.6%\n",
      "step  350 | loss 0.143 | val acc  94.0%\n",
      "step  351 | loss 0.122 | val acc  93.2%\n",
      "step  352 | loss 0.140 | val acc  95.0%\n",
      "step  353 | loss 0.131 | val acc  93.5%\n",
      "step  354 | loss 0.129 | val acc  93.2%\n",
      "step  355 | loss 0.131 | val acc  93.9%\n",
      "step  356 | loss 0.128 | val acc  94.6%\n",
      "step  357 | loss 0.133 | val acc  93.3%\n",
      "step  358 | loss 0.137 | val acc  94.5%\n",
      "step  359 | loss 0.133 | val acc  95.0%\n",
      "step  360 | loss 0.133 | val acc  94.8%\n",
      "step  361 | loss 0.131 | val acc  94.8%\n",
      "step  362 | loss 0.129 | val acc  93.9%\n",
      "step  363 | loss 0.123 | val acc  95.4%\n",
      "step  364 | loss 0.125 | val acc  95.2%\n",
      "step  365 | loss 0.125 | val acc  95.7%\n",
      "step  366 | loss 0.130 | val acc  95.3%\n",
      "step  367 | loss 0.125 | val acc  95.9%\n",
      "step  368 | loss 0.123 | val acc  95.0%\n",
      "step  369 | loss 0.132 | val acc  95.5%\n",
      "step  370 | loss 0.134 | val acc  93.4%\n",
      "step  371 | loss 0.128 | val acc  94.7%\n",
      "step  372 | loss 0.126 | val acc  93.8%\n",
      "step  373 | loss 0.136 | val acc  94.3%\n",
      "step  374 | loss 0.126 | val acc  94.1%\n",
      "step  375 | loss 0.131 | val acc  93.9%\n",
      "step  376 | loss 0.133 | val acc  96.4%\n",
      "step  377 | loss 0.125 | val acc  95.1%\n",
      "step  378 | loss 0.119 | val acc  96.2%\n",
      "step  379 | loss 0.129 | val acc  95.4%\n",
      "step  380 | loss 0.122 | val acc  94.3%\n",
      "step  381 | loss 0.125 | val acc  95.5%\n",
      "step  382 | loss 0.125 | val acc  95.9%\n",
      "step  383 | loss 0.135 | val acc  94.9%\n",
      "step  384 | loss 0.126 | val acc  95.5%\n",
      "step  385 | loss 0.121 | val acc  95.4%\n",
      "step  386 | loss 0.125 | val acc  96.4%\n",
      "step  387 | loss 0.117 | val acc  95.9%\n",
      "step  388 | loss 0.121 | val acc  97.0%\n",
      "step  389 | loss 0.118 | val acc  94.1%\n",
      "step  390 | loss 0.116 | val acc  95.0%\n",
      "step  391 | loss 0.129 | val acc  94.3%\n",
      "step  392 | loss 0.127 | val acc  94.0%\n",
      "step  393 | loss 0.120 | val acc  96.8%\n",
      "step  394 | loss 0.120 | val acc  95.9%\n",
      "step  395 | loss 0.113 | val acc  96.4%\n",
      "step  396 | loss 0.119 | val acc  96.2%\n",
      "step  397 | loss 0.116 | val acc  96.6%\n",
      "step  398 | loss 0.113 | val acc  95.2%\n",
      "step  399 | loss 0.114 | val acc  96.1%\n",
      "step  400 | loss 0.121 | val acc  95.5%\n",
      "step  401 | loss 0.121 | val acc  95.8%\n",
      "step  402 | loss 0.122 | val acc  96.3%\n",
      "step  403 | loss 0.122 | val acc  95.2%\n",
      "step  404 | loss 0.118 | val acc  95.6%\n",
      "step  405 | loss 0.111 | val acc  97.0%\n",
      "step  406 | loss 0.113 | val acc  95.9%\n",
      "step  407 | loss 0.107 | val acc  95.4%\n",
      "step  408 | loss 0.117 | val acc  95.0%\n",
      "step  409 | loss 0.116 | val acc  95.9%\n",
      "step  410 | loss 0.123 | val acc  95.6%\n",
      "step  411 | loss 0.120 | val acc  95.5%\n",
      "step  412 | loss 0.110 | val acc  97.2%\n",
      "step  413 | loss 0.125 | val acc  96.1%\n",
      "step  414 | loss 0.119 | val acc  95.7%\n",
      "step  415 | loss 0.106 | val acc  95.2%\n",
      "step  416 | loss 0.115 | val acc  94.8%\n",
      "step  417 | loss 0.116 | val acc  96.4%\n",
      "step  418 | loss 0.105 | val acc  94.8%\n",
      "step  419 | loss 0.114 | val acc  96.1%\n",
      "step  420 | loss 0.113 | val acc  93.8%\n",
      "step  421 | loss 0.123 | val acc  95.1%\n",
      "step  422 | loss 0.119 | val acc  94.3%\n",
      "step  423 | loss 0.118 | val acc  94.2%\n",
      "step  424 | loss 0.120 | val acc  94.5%\n",
      "step  425 | loss 0.119 | val acc  94.2%\n",
      "step  426 | loss 0.120 | val acc  96.1%\n",
      "step  427 | loss 0.113 | val acc  94.3%\n",
      "step  428 | loss 0.109 | val acc  95.0%\n",
      "step  429 | loss 0.103 | val acc  94.3%\n",
      "step  430 | loss 0.113 | val acc  96.4%\n",
      "step  431 | loss 0.112 | val acc  95.9%\n",
      "step  432 | loss 0.111 | val acc  93.1%\n",
      "step  433 | loss 0.112 | val acc  96.5%\n",
      "step  434 | loss 0.108 | val acc  96.4%\n",
      "step  435 | loss 0.113 | val acc  97.3%\n",
      "step  436 | loss 0.109 | val acc  95.5%\n",
      "step  437 | loss 0.109 | val acc  97.6%\n",
      "step  438 | loss 0.115 | val acc  95.4%\n",
      "step  439 | loss 0.110 | val acc  95.8%\n",
      "step  440 | loss 0.111 | val acc  96.5%\n",
      "step  441 | loss 0.107 | val acc  95.9%\n",
      "step  442 | loss 0.108 | val acc  95.2%\n",
      "step  443 | loss 0.108 | val acc  96.5%\n",
      "step  444 | loss 0.098 | val acc  94.8%\n",
      "step  445 | loss 0.117 | val acc  96.1%\n",
      "step  446 | loss 0.110 | val acc  95.2%\n",
      "step  447 | loss 0.106 | val acc  95.2%\n",
      "step  448 | loss 0.113 | val acc  94.8%\n",
      "step  449 | loss 0.106 | val acc  94.0%\n",
      "step  450 | loss 0.113 | val acc  95.0%\n",
      "step  451 | loss 0.098 | val acc  93.5%\n",
      "step  452 | loss 0.113 | val acc  95.4%\n",
      "step  453 | loss 0.107 | val acc  95.1%\n",
      "step  454 | loss 0.113 | val acc  93.3%\n",
      "step  455 | loss 0.108 | val acc  96.2%\n",
      "step  456 | loss 0.113 | val acc  94.6%\n",
      "step  457 | loss 0.108 | val acc  97.0%\n",
      "step  458 | loss 0.099 | val acc  95.4%\n",
      "step  459 | loss 0.109 | val acc  94.9%\n",
      "step  460 | loss 0.111 | val acc  96.7%\n",
      "step  461 | loss 0.100 | val acc  95.5%\n",
      "step  462 | loss 0.107 | val acc  97.1%\n",
      "step  463 | loss 0.105 | val acc  96.0%\n",
      "step  464 | loss 0.100 | val acc  96.1%\n",
      "step  465 | loss 0.098 | val acc  95.4%\n",
      "step  466 | loss 0.106 | val acc  96.6%\n",
      "step  467 | loss 0.102 | val acc  95.4%\n",
      "step  468 | loss 0.111 | val acc  95.7%\n",
      "step  469 | loss 0.099 | val acc  96.8%\n",
      "step  470 | loss 0.100 | val acc  96.6%\n",
      "step  471 | loss 0.099 | val acc  95.6%\n",
      "step  472 | loss 0.104 | val acc  96.9%\n",
      "step  473 | loss 0.100 | val acc  95.4%\n",
      "step  474 | loss 0.102 | val acc  97.2%\n",
      "step  475 | loss 0.097 | val acc  95.9%\n",
      "step  476 | loss 0.106 | val acc  95.9%\n",
      "step  477 | loss 0.105 | val acc  96.2%\n",
      "step  478 | loss 0.098 | val acc  96.8%\n",
      "step  479 | loss 0.105 | val acc  96.4%\n",
      "step  480 | loss 0.100 | val acc  96.0%\n",
      "step  481 | loss 0.103 | val acc  97.1%\n",
      "step  482 | loss 0.093 | val acc  96.0%\n",
      "step  483 | loss 0.104 | val acc  96.2%\n",
      "step  484 | loss 0.101 | val acc  96.6%\n",
      "step  485 | loss 0.103 | val acc  98.0%\n",
      "step  486 | loss 0.101 | val acc  96.7%\n",
      "step  487 | loss 0.093 | val acc  95.5%\n",
      "step  488 | loss 0.107 | val acc  95.5%\n",
      "step  489 | loss 0.102 | val acc  96.5%\n",
      "step  490 | loss 0.103 | val acc  93.2%\n",
      "step  491 | loss 0.109 | val acc  97.0%\n",
      "step  492 | loss 0.091 | val acc  97.5%\n",
      "step  493 | loss 0.095 | val acc  95.1%\n",
      "step  494 | loss 0.095 | val acc  95.8%\n",
      "step  495 | loss 0.099 | val acc  95.5%\n",
      "step  496 | loss 0.100 | val acc  95.5%\n",
      "step  497 | loss 0.107 | val acc  95.7%\n",
      "step  498 | loss 0.101 | val acc  94.8%\n",
      "step  499 | loss 0.098 | val acc  93.4%\n",
      "step  500 | loss 0.102 | val acc  96.4%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if True:\n",
    "    # --------------------------\n",
    "    # Model\n",
    "    # --------------------------\n",
    "    model = PointerNetwork(hidden_dim=64).to(DEVICE)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    steps = 500\n",
    "    batch_sz = 256\n",
    "    seq_len = 10\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        # print (\"step\", step)\n",
    "        model.train() \n",
    "        inputs, targets = gen_batch(batch_sz, seq_len)\n",
    "        logits = model(inputs, targets)\n",
    "        logits = logits.view(-1, seq_len) # (B*T, T) samples by logits\n",
    "        targets = targets.view(-1) # (B*T) by class\n",
    "        # print (\"logits\", logits, logits.shape)\n",
    "        # print (\"targets\", targets, targets.shape)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        # print (\"loss\", loss)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_inp, val_tgt = gen_batch(batch_sz, seq_len)\n",
    "                val_logits = model(val_inp)  # no teacher forcing\n",
    "                preds = val_logits.argmax(-1)\n",
    "                accuracy = (preds == val_tgt).float().mean().item()\n",
    "                print(\n",
    "                    f\"step {step:>4} | loss {loss.item():.3f} | val acc {accuracy*100:5.1f}%\"\n",
    "                )\n",
    "\n",
    "# inputs, targets, logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
