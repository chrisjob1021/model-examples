{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, input_dim = 1):\n",
    "        super().__init__()\n",
    "        # input_dim is the size of each element’s feature vector—what the encoder LSTM sees at every time-step.\n",
    "        # In the little “sort a list of real numbers” demo we built, each token is just a single scalar (e.g., 0.42).\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
    "        # With batch_first=True: The input and output tensors are expected to be of shape (batch, seq_len, input_size). \n",
    "        # With bidirectional=True: The LSTM processes the input sequence in both forward and backward directions. \n",
    "        #   This means for each time step, the output contains information from both past and future contexts.\n",
    "        # The output of the bidirectional LSTM is twice the hidden dimension.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1) # add feature dimension -> (B, T, 1)\n",
    "        # print(x)\n",
    "        # x is the input to the LSTM, which is a tensor of shape (batch_size, seq_len, input_size).\n",
    "        # LSTM expects a 3D input tensor of shape (batch_size, seq_len, input_size).\n",
    "        h, _ = self.lstm(x)\n",
    "        # h is the output of the LSTM, which is a tensor of shape (batch_size, seq_len, hidden_dim * 2).\n",
    "        # The final layer of the encoder is a linear layer that maps the output of the LSTM to a vector of size hidden_dim.\n",
    "        return h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=hidden_dim * 2, hidden_size=hidden_dim)\n",
    "        self.W1 = nn.Linear(2 * hidden_dim, hidden_dim, bias=False) # input is bidirectional\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, enc_out, targets=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        enc_out : (B, T, 2H)\n",
    "        targets  : (B, T) or None  (teacher forcing indices)\n",
    "\n",
    "        B = batch size\n",
    "        T = sequence length\n",
    "        H = hidden dimension\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : (B, T, T)  --- unnormalised pointer scores\n",
    "        \"\"\"\n",
    "\n",
    "        B, T, _ = enc_out.size()\n",
    "        h_t = torch.zeros(B, self.hidden_dim, device=enc_out.device)\n",
    "        c_t = torch.zeros_like(h_t)\n",
    "\n",
    "        logits = []\n",
    "        mask = torch.zeros(B, T, device=enc_out.device)\n",
    "\n",
    "        enc_proj = self.W1(enc_out) # (B, T, H)\n",
    "        enc_out_idx = torch.arange(B, device=enc_out.device) # (B) This creates a tensor [0, 1, ..., B-1] to index each batch.\n",
    "\n",
    "        for i in range(T):\n",
    "            if i == 0:\n",
    "                # first time step, use the mean of the encoder outputs\n",
    "                ctx = enc_out.mean(dim=1) # (B, 2H)\n",
    "            else:\n",
    "                # subsequent time steps, use the previously chosen embedding as context\n",
    "                idx = targets[:, i-1] if targets is not None else prev_idx\n",
    "                ctx = enc_out[enc_out_idx, idx] # (B, H) For each item in the batch, select the encoder output at the position given by idx.\n",
    "                                                #        This gives you a context vector for each batch item, based on the previously selected position.\n",
    "            \n",
    "            # LSTMCell\n",
    "            # Inputs: input, (h_0, c_0)\n",
    "            # input of shape (batch, input_size) or (input_size): tensor containing input features\n",
    "            # h_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial hidden state\n",
    "            # c_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial cell state\n",
    "\n",
    "            # Outputs: (h_1, c_1)\n",
    "            # h_1 of shape (batch, hidden_size) or (hidden_size): tensor containing the next hidden state\n",
    "            # c_1 of shape (batch, hidden_size) or (hidden_size): tensor containing the next cell state\n",
    "\n",
    "            # current decoder hidden- and cell-state BEFORE we look at position i\n",
    "            h_t, c_t = self.lstm_cell(ctx, (h_t, c_t))   # (B, H)\n",
    "            W2_proj = self.W2(h_t).unsqueeze(1) # (B, 1, H)\n",
    "            tanh = torch.tanh(enc_proj + W2_proj) # (B, T, H)\n",
    "            u_i = self.v(tanh).squeeze(-1) # (B, T)\n",
    "            u_i = u_i - 1e9 * mask\n",
    "            # print (\"u_i\", u_i)\n",
    "            logits.append(u_i)\n",
    "\n",
    "            prev_idx = torch.argmax(u_i, dim=-1)\n",
    "            # print (\"prev_idx\", prev_idx)\n",
    "            mask[enc_out_idx, prev_idx] = 1 # mask the selected index for future predictions\n",
    "\n",
    "        # In this context, torch.stack(logits, dim=1) is used to combine a list of tensors (each representing the logits at a different decoding step) into a single tensor with a new dimension.\n",
    "        # If you have T decoding steps, and each u_i is shape (B, T), then after stacking, you get a tensor of shape (B, T, T).\n",
    "        logits = torch.stack(logits, dim=1) # (B, T, T)\n",
    "        return logits\n",
    "\n",
    "class PointerNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        enc_out = self.encoder(x) # (B, T, 2H)\n",
    "        # print (\"inputs\", x)\n",
    "        # print (\"targets\", targets)\n",
    "        logits = self.decoder(enc_out, targets) # (B, T, T)\n",
    "        return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Synthetic dataset helpers\n",
    "# --------------------------\n",
    "def gen_batch(batch_sz, seq_len = 5):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    inputs : (B, T)  float32  -- unsorted numbers\n",
    "    targets: (B, T)  long     -- permutation (indices) that would sort each row ascending\n",
    "    \"\"\"\n",
    "    inputs = torch.rand(batch_sz, seq_len)\n",
    "    targets = torch.argsort(inputs, dim=1)  # ascending order indices\n",
    "    return inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  100 | loss 412499968.000 | val acc  17.5%\n",
      "step  200 | loss 400000000.000 | val acc  18.8%\n",
      "step  300 | loss 431249984.000 | val acc  25.6%\n",
      "step  400 | loss 406250016.000 | val acc  11.9%\n",
      "step  500 | loss 399999936.000 | val acc  24.4%\n",
      "step  600 | loss 350000000.000 | val acc  20.0%\n",
      "step  700 | loss 449999968.000 | val acc  20.0%\n",
      "step  800 | loss 418750016.000 | val acc  25.0%\n",
      "step  900 | loss 387500000.000 | val acc  24.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m model.train() \n\u001b[32m     20\u001b[39m inputs, targets = gen_batch(batch_sz, seq_len)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m logits = logits.view(-\u001b[32m1\u001b[39m, seq_len) \u001b[38;5;66;03m# (B*T, T) samples by logits\u001b[39;00m\n\u001b[32m     23\u001b[39m targets = targets.view(-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# (B*T) by class\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/model-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/model-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mPointerNetwork.forward\u001b[39m\u001b[34m(self, x, targets)\u001b[39m\n\u001b[32m    104\u001b[39m enc_out = \u001b[38;5;28mself\u001b[39m.encoder(x) \u001b[38;5;66;03m# (B, T, 2H)\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# print (\"inputs\", x)\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# print (\"targets\", targets)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/model-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/model-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, enc_out, targets)\u001b[39m\n\u001b[32m     66\u001b[39m     ctx = enc_out[enc_out_idx, idx] \u001b[38;5;66;03m# (B, H) For each item in the batch, select the encoder output at the position given by idx.\u001b[39;00m\n\u001b[32m     67\u001b[39m                                     \u001b[38;5;66;03m#        This gives you a context vector for each batch item, based on the previously selected position.\u001b[39;00m\n\u001b[32m     68\u001b[39m \n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# LSTMCell\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# current decoder hidden- and cell-state BEFORE we look at position i\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m h_t, c_t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (B, H)\u001b[39;00m\n\u001b[32m     81\u001b[39m W2_proj = \u001b[38;5;28mself\u001b[39m.W2(h_t).unsqueeze(\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# (B, 1, H)\u001b[39;00m\n\u001b[32m     82\u001b[39m tanh = torch.tanh(enc_proj + W2_proj) \u001b[38;5;66;03m# (B, T, H)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/model-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/model-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/model-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1706\u001b[39m, in \u001b[36mLSTMCell.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1704\u001b[39m     hx = (hx[\u001b[32m0\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m), hx[\u001b[32m1\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[32m-> \u001b[39m\u001b[32m1706\u001b[39m ret = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   1716\u001b[39m     ret = (ret[\u001b[32m0\u001b[39m].squeeze(\u001b[32m0\u001b[39m), ret[\u001b[32m1\u001b[39m].squeeze(\u001b[32m0\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if True:\n",
    "    # --------------------------\n",
    "    # Model\n",
    "    # --------------------------\n",
    "    model = PointerNetwork(hidden_dim=128).to(DEVICE)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    steps = 10000\n",
    "    batch_sz = 32\n",
    "    seq_len = 5\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        # print (\"step\", step)\n",
    "        model.train() \n",
    "        inputs, targets = gen_batch(batch_sz, seq_len)\n",
    "        logits = model(inputs, targets)\n",
    "        logits = logits.view(-1, seq_len) # (B*T, T) samples by logits\n",
    "        targets = targets.view(-1) # (B*T) by class\n",
    "        # print (\"logits\", logits, logits.shape)\n",
    "        # print (\"targets\", targets, targets.shape)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        # print (\"loss\", loss)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_inp, val_tgt = gen_batch(batch_sz, seq_len)\n",
    "                val_logits = model(val_inp)  # no teacher forcing\n",
    "                preds = val_logits.argmax(-1)\n",
    "                accuracy = (preds == val_tgt).float().mean().item()\n",
    "                print(\n",
    "                    f\"step {step:>4} | loss {loss.item():.3f} | val acc {accuracy*100:5.1f}%\"\n",
    "                )\n",
    "\n",
    "# inputs, targets, logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
